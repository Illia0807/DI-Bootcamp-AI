{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Open Source Levels Reflection"
      ],
      "metadata": {
        "id": "un_ppT1fRt9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fully Open** : This level implies that all components of the model are available: the source code of the model design, the pre-trained weights, the code, the resource for training the models, and even (sometimes) the training data or the methodology for collecting and processing it. In essence, this is full transparency and accessibility of the entire model development and improvement stack.\n",
        "\n",
        "**Weights Released:**\n",
        "At this level, the developer releases the pre-trained model weights and, typically, its architecture. However, the source code for a full retraining from scratch, the original training data, and the detailed training methodology may not be available. This means that you get a \"ready to use\" model, but not all the tools to deeply rework it.\n",
        "\n",
        "**Architecture Only:** This is the most limited level of openness. Only the design or structure of the model is available - its \"blueprints\". There are no pre-trained weights, no training data or code. To use this model, you will need to train it yourself from scratch, which is a resource-intensive and expensive task.\n",
        "\n",
        "2.\n",
        "**Fully Open:**\n",
        "\n",
        "What is open? The source code of the architecture, the pre-trained weights, the training code, the training data/methodology – the entire stack.\n",
        "\n",
        "What is allowed and not allowed? You are free to inspect every aspect of the model, modify it, retrain it from scratch on any data, adapt its behavior, and use it for any purpose (commercial or non-commercial), subject only to the general terms of the license. You have full control and insight.\n",
        "\n",
        "**Weights Released:**\n",
        "\n",
        "What is open? The pre-trained weights and the model architecture.\n",
        "\n",
        "What is allowed and not allowed? You can use the model for inference, fine-tune it on your own data, and run it locally. However, you cannot fully retrain it from scratch (without access to the original training data/code) or understand all the internal mechanisms of its formation, which limits deep modification or audit of the training process.\n",
        "\n",
        "**Architecture Only:**\n",
        "\n",
        "What is open? Only the structure or design of the model.\n",
        "\n",
        "What can and cannot be done? You can understand how the model is conceptually built. You cannot use it for inference or retraining, as there are no weights. You can implement and train the model from scratch on your data, but this requires significant computing resources, time, and expert knowledge.\n",
        "\n",
        "Fully Open You can inspect, modify, and retrain the entire model, with full transparency and control over all aspects of its creation and operation. This provides maximum flexibility for customization and adaptation.\n",
        "Weights Released You have pre-trained weights for inference and fine-tuning on your data, but do not have full access to the training code or the original data. This allows you to tailor the model to specific problems, but without the ability to fundamentally change its training process or architecture.\n",
        "Architecture Only You only know the structure of the model, but do not have pre-trained weights. This means you must train the model from scratch on your data, which requires enormous computational resources and data, making this layer the least \"production-ready\"."
      ],
      "metadata": {
        "id": "9JM2YmtrS1Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Fully Open:** This is like having a full set of blueprints, all the building materials, and access to the factory where they are made. You can build anything, any way you want, and understand every screw.\n",
        "\n",
        "**Weights Released:** This is like getting a finished building with furniture, and you are allowed to move the furniture around a bit or paint the walls.\n",
        "\n",
        "**Architecture Only:** It's like getting only a house plan without materials and without the house itself. You have to find all the materials and build it from scratch."
      ],
      "metadata": {
        "id": "Ai9OIfv8TpNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\n",
        "A fully open level of openness is absolutely essential. This level supports domain-specific fine-tuning, as it ensures full transparency and control over the model — which is critically important for complying with strict data privacy regulations (such as HIPAA) and enabling auditability in medical applications."
      ],
      "metadata": {
        "id": "fh8af7fA8jW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: License Check for SaaS Use"
      ],
      "metadata": {
        "id": "sCGq8LXT-DD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For mistralai/Mistral-7B-Instruct-v0.2 (Apache 2.0 License):**\n",
        "\n",
        "Commercial Use: Explicitly Permitted.\n",
        "\n",
        "Quick Note: Permission is granted to freely use, modify, and distribute the software (including models) for commercial purposes. The copyright notice and license text must be preserved. This is one of the more permissive licenses.\n",
        "\n",
        "**For meta-llama/Llama-2-7b-chat-hf (Llama 2 Community License):**\n",
        "\n",
        "Commercial Use: Conditionally Permitted.\n",
        "\n",
        "Quick Note: Permitted for most commercial and research purposes, unless your product or service has 700 million or more monthly active users (MAU). Above that threshold, you must contact Meta for a separate commercial license. This makes it a conditionally permissive or proprietary license for very large enterprises."
      ],
      "metadata": {
        "id": "V4RyRk64_XVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For mistralai/Mistral-7B-Instruct-v0.2 (Apache 2.0 License):**\n",
        "\n",
        "Limitation #1: License Attribution/Copying Requirement: You must include a copy of the Apache 2.0 License and the copyright notice in all copies or substantial portions of the software and in any documentation.\n",
        "\n",
        "Limitation #2: Disclaimer of Warranty: The software is provided \"as is\", without warranty of any kind.\n",
        "\n",
        "Limitation #3: Patent License: You are granted a patent license under the patents owned by the authors, which protects you from patent lawsuits based on your use of the software.\n",
        "\n",
        "==================================\n",
        "\n",
        "**For meta-llama/Llama-2-7b-chat-hf (Llama 2 Community License):**\n",
        "\n",
        "Restriction #1: Maximum Number of Users (MAU): If your product exceeds 700 million monthly active users, you must obtain a commercial license from Meta.\n",
        "\n",
        "Restriction #2: No Use to Improve Other LLMs (except Llama 2): You may not use Llama 2 (or its derivatives) to train or improve other language models unless they are Llama 2 or its derivatives.\n",
        "\n",
        "Restriction #3: Acceptable Use Policy: Use of the model must comply with Meta's Acceptable Use Policy (e.g. no use for illegal activities, hate speech, malicious content, use in risky areas such as military applications without special permission, etc.).\n",
        "\n",
        "Restriction #4: Notification and Benchmarking Requirement (for Meta competitors): If you are a competitor of Meta in the LLM products or services, you must notify Meta of your use and provide them with benchmarking results.\n",
        "\n",
        "Restriction #5: Attribution Requirement: If you distribute Llama 2 or products built on it, you must clearly state \"Built with Llama\" (or similar) on your website/product.\n",
        "\n",
        "(Export control is not part of the license itself, but as with any software, it is subject to US law since Meta is a US company. This is an important point for SaaS that operates globally.)"
      ],
      "metadata": {
        "id": "CIiAFI6y_umV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [ ] **mistralai/Mistral-7B-Instruct-v0.2**\n",
        "  - Type of license: Permissive (разрешительная)\n",
        "  - [x] License name: Apache 2.0\n",
        "  - Commercial use allowed:\n",
        "    - [x] Yes\n",
        "    - [ ] No\n",
        "    - [ ] Conditional\n",
        "  - Restrictions:\n",
        "    - [x] Requires inclusion of Apache 2.0 license text and copyright notice in all copies/documentation.\n",
        "    - [x] Provided \"as is\" without warranty.\n",
        "    - [x] Includes explicit patent license.\n",
        "\n",
        "- [ ] **meta-llama/Llama-2-7b-chat-hf**\n",
        "  - Type of license: Conditional Permissive / Proprietary (условно-разрешительная / проприетарная)\n",
        "  - [x] License name: Llama 2 Community License\n",
        "  - Commercial use allowed:\n",
        "    - [ ] Yes\n",
        "    - [ ] No\n",
        "    - [x] Conditional (Allowed unless Monthly Active Users exceed 700 million)\n",
        "  - Restrictions:\n",
        "    - [x] User limit: Over 700M MAU requires separate commercial license from Meta.\n",
        "    - [x] Cannot be used to train or improve other language models (excluding Llama 2 or derivatives).\n",
        "    - [x] Must comply with Meta's Acceptable Use Policy (prohibits illegal, harmful, dangerous activities, etc.).\n",
        "    - [x] If a Meta competitor (in LLM products/services), must notify Meta and provide benchmarking results.\n",
        "    - [x] Requires explicit attribution (e.g., \"Built with Llama\") on product/website if distributing."
      ],
      "metadata": {
        "id": "D3MDurSCAXOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: LLM Matchmaker Challenge"
      ],
      "metadata": {
        "id": "p86eSLTFAdC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LegalTech Legal chatbot Runs only on CPU, logically strict response\n",
        "EdTech Educational system Mathematical logic, runs on weak laptops\n",
        "NGO Global organization Supports more than 5 languages"
      ],
      "metadata": {
        "id": "nViRFBc_DGgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LegalTech:**\n",
        "Filters: Quantized (for CPU compatibility), Small size (for CPU speed), Logic / Reasoning tags\n",
        "\n",
        "** EdTech:**\n",
        " Filters: Small (e.g. <3B parameters), Quantized (for low-end laptops), Math/Logic tags.\n",
        "\n",
        "** Global NGO:**\n",
        "Filters: language:multilingual, preferably with good performance on FLORES-200."
      ],
      "metadata": {
        "id": "tZYjXp_NEbyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LegalTech (CPU-only, logic-heavy chatbot):**\n",
        "\n",
        "*mistralai/Mistral-7B-Instruct-v0.2*\n",
        "\n",
        "Parameters: 7B\n",
        "\n",
        "Architecture: Mistral (Decoder-only Transformer, GQA, SWA)\n",
        "\n",
        "Quantization: GGUF available (Q4_K_M, Q5_K_M, etc.)\n",
        "\n",
        "Benchmarks: Excellent general reasoning, good on MMLU, Hellaswag, ARC. For logic, they often fine-tune on datasets like OpenHermes.\n",
        "\n",
        "*HuggingFaceH4/zephyr-7b-beta (fine-tuned Mistral 7B)*\n",
        "\n",
        "Parameters: 7B\n",
        "\n",
        "Architecture: Mistral (fine-tuned)\n",
        "\n",
        "Quantization: GGUF available\n",
        "\n",
        "Benchmarks: High scores on MT-Bench and AlpacaEval (general instruction following, dialogue), good reasoning.\n",
        "\n",
        "*teknium/OpenHermes-2.5-Mistral-7B (fine-tuned Mistral 7B)*\n",
        "\n",
        "Parameters: 7B\n",
        "\n",
        "Architecture: Mistral (fine-tuned)\n",
        "\n",
        "Quantization: GGUF available\n",
        "\n",
        "Benchmarks: Good on GPT4All, AGIEval, TruthfulQA, and improved coding abilities, which often correlates with logic."
      ],
      "metadata": {
        "id": "RlVL2B5gE4U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EdTech (math/logic focus on low-end laptops):**\n",
        "\n",
        "*microsoft/Phi-2*\n",
        "\n",
        "Parameters: 2.7B\n",
        "\n",
        "Architecture: Transformer (dense decoder-only)\n",
        "\n",
        "Quantization: GGUFs available (Q2_K, Q3_K, Q4_K_M, etc.)\n",
        "\n",
        "Benchmarks: High for its size on common sense, language understanding, math, and logical reasoning benchmarks. Very compact.\n",
        "\n",
        "*TinyLlama/TinyLlama-1.1B-Chat-v1.0*\n",
        "\n",
        "Parameters: 1.1B\n",
        "\n",
        "Architecture: Llama 2\n",
        "\n",
        "Quantization: GGUFs available (from Q2_K to Q8_0)\n",
        "\n",
        "Benchmarks: Basic performance, but extremely small size and high inference speed.\n",
        "\n",
        "*OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 (smaller Pythia models)*\n",
        "\n",
        "Parameters: E.g. Pythia-2.8B\n",
        "\n",
        "Architecture: Pythia (Decoder-only)\n",
        "\n",
        "Quantization: GGUF available\n",
        "\n",
        "Benchmarks: General capabilities, good for basic tasks on limited resources."
      ],
      "metadata": {
        "id": "XI6gc1nDFcl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Global NGO (supports 5+ languages well):**\n",
        "\n",
        "*Qwen/Qwen1.5-7B-Chat*\n",
        "\n",
        "Parameters: 7B\n",
        "\n",
        "Architecture: Transformer (SwiGLU, GQA, SWA, improved tokenizer)\n",
        "\n",
        "Quantization: GGUF, AWQ, GPTQ available\n",
        "\n",
        "Benchmarks: Excellent multilingual support (Chinese, English, Spanish, Russian, French and others), good results on C-Eval (Chinese), MMLU.\n",
        "\n",
        "*bigscience/bloom-7b1*\n",
        "\n",
        "Parameters: 7.1B\n",
        "\n",
        "Architecture: BLOOM (Transformer)\n",
        "\n",
        "Quantization: GGUF available\n",
        "\n",
        "Benchmarks: Originally developed for a wide range of languages (46 natural, 13 programming languages), good results on FLORES-200.\n",
        "\n",
        "*facebook/nllb-200-distilled-600M (although it is not exactly LLM, but more of a translator, but it is relevant for multilingual support)*\n",
        "\n",
        "Parameters: 600M\n",
        "\n",
        "Architecture: Transformer\n",
        "\n",
        "Quantization: Available\n",
        "\n",
        "Benchmarks: Specialized for translation, high rates on FLORES-200 (200 languages). If we are talking purely about translation, then this is the best choice. If you need a \"dialogue\", then Qwen or BLOOM."
      ],
      "metadata": {
        "id": "XLFa7blzF8oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate Against Criteria"
      ],
      "metadata": {
        "id": "TB2bD5rWGN5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LegalTech:**\n",
        "\n",
        "mistralai/Mistral-7B-Instruct-v0.2 (GGUF): Very strong in logic and instruction following. GGUF allows for efficient CPU performance.\n",
        "\n",
        "teknium/OpenHermes-2.5-Mistral-7B (GGUF): Even stronger in reasoning and coding (which is good for logic), a good base for Mistral.\n",
        "\n",
        "Comparison: OpenHermes-2.5-Mistral-7B, being a retrained version of Mistral, often shows improved reasoning and instruction following abilities, which are critical for LegalTech.\n",
        "\n",
        "**EdTech:**\n",
        "\n",
        "microsoft/Phi-2 (GGUF): Outstanding performance for its size (2.7B), with good scores in logic and math. Ideal for low-end laptops due to its small size.\n",
        "\n",
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0 (GGUF): The smallest of the bunch, offering the fastest speed and lowest RAM requirements, but its math and logic skills may be more basic than Phi-2.\n",
        "\n",
        "Comparison: Phi-2 clearly outperforms TinyLlama in math and logic while still being very small, making it a better choice for EdTech where subject-specific answer quality is critical.\n",
        "\n",
        "**Global NGO:**\n",
        "\n",
        "Qwen/Qwen1.5-7B-Chat (GGUF): Strong multilingual model, especially good for Asian languages, as well as Russian, English, etc. Good overall performance and conversational ability.\n",
        "\n",
        "bigscience/bloom-7b1 (GGUF): Designed with multilingualism in mind, covers 46 natural languages. May be less polished for chatbots than Qwen, but its language coverage is impressive.\n",
        "\n",
        "Comparison: Qwen1.5-7B-Chat offers not only a wide language coverage, but also high-quality dialogue generation and problem-solving capabilities, which is more versatile for NGOs than pure translation (like NLLB) or a less focused chatbot (like BLOOM)."
      ],
      "metadata": {
        "id": "Q7V_jdEzGPDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Select the Best Fit"
      ],
      "metadata": {
        "id": "_0O_Plg5Gej-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LegalTech: teknium/OpenHermes-2.5-Mistral-7B**\n",
        "\n",
        "Rationale: Its Mistral base delivers excellent performance for CPU inference (via GGUF), and retraining on high-quality datasets (including code) significantly improves reasoning and logic abilities, which are critical for LegalTech.\n",
        "\n",
        "**EdTech: microsoft/Phi-2**\n",
        "\n",
        "Rationale: With 2.7B parameters, Phi-2 offers surprisingly strong math and logic abilities for its ultra-compact size. This makes it ideal for running on low-end laptops while maintaining the high accuracy needed for training.\n",
        "\n",
        "**Global NGO: Qwen/Qwen1.5-7B-Chat**\n",
        "\n",
        "Rationale: This model stands out for its true multilingual support, especially thanks to its improved tokenizer and training on extensive multilingual data. It demonstrates strong performance in dialogue and understanding across multiple languages, which is essential for a global NGO."
      ],
      "metadata": {
        "id": "ut8v2cFVGf9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LegalTech** Fast model for logic-heavy chatbot on CPU teknium/OpenHermes-2.5-Mistral-7B (Optimized for logic and CPU inference)\n",
        "\n",
        "**EdTech Logic**/math-focused LLM on low-end laptops microsoft/Phi-2 (Excellent math/logic performance for a small footprint)\n",
        "\n",
        "**Global NGO Model** that speaks 5+ languages well Qwen/Qwen1.5-7B-Chat (Strong multilingual support and conversational capabilities)"
      ],
      "metadata": {
        "id": "UEN8FPfPGqT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Local Readiness Audit"
      ],
      "metadata": {
        "id": "xpzAS7ZAHjRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OS Name: Microsoft Windows 10 Home, Build 19045\n",
        "\n",
        "System Type: x64-based PC\n",
        "\n",
        "Processor(s): Intel64 Family 6 Model 126 Stepping 5 GenuineIntel (~991 MHz, although this is the base frequency, the actual will be higher under load)\n",
        "\n",
        "Total physical memory (RAM): 7,931 MB (~7.75 GB)\n",
        "\n",
        "Available physical memory (RAM): 1,144 MB\n",
        "\n",
        "Virtual memory: Max. size: 21,403 MB\n",
        "\n",
        "Virtual memory: Used: 17,572 MB\n",
        "\n",
        "Pagefile location: D:\\pagefile.sys\n",
        "\n",
        "Free space on drive C: 21,656,031,232 bytes (~20.17 GB)\n",
        "\n",
        "Free space on drive D: 143,504,539,648 bytes (~133.64 GB)\n",
        "\n",
        "Total available free space on drives: ~153.81 GB"
      ],
      "metadata": {
        "id": "riuGYkygHk4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirement\tYour System Specs\tMeets Requirement?**\n",
        "RAM (≥ 16 GB)\t7.75 GB\t❌\n",
        "\n",
        "Free Disk Space (≥ 40 GB)\t153.81 GB\t✅\n",
        "\n",
        "OS (Linux/WSL2)\tWindows 10 Домашняя\t❌\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QSjiusg9IBUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Check Against Requirements"
      ],
      "metadata": {
        "id": "fndkf6b2IM0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the completed table:\n",
        "\n",
        "RAM (≥ 16 GB): ❌ (Your 7.75 GB is significantly less than the required 16 GB).\n",
        "\n",
        "Free Disk Space (≥ 40 GB): ✅ (Your 153.81 GB on drives C and D together is significantly more than 40 GB).\n",
        "\n",
        "OS (Linux/WSL2): ❌ (You have Windows 10 Home, and running llama.cpp in a native Windows environment may require more effort or compiling specific versions. For simplicity and consistency with the exercise, Linux or WSL2 is required)."
      ],
      "metadata": {
        "id": "PDn8OabQIQtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Identify Upgrade Needs"
      ],
      "metadata": {
        "id": "pjPxDAKgIfTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAM: ❌ (7.75 GB)\n",
        "\n",
        "Required Upgrade: You absolutely need to increase your RAM to 16 GB or more. This is the most critical limitation for running most 7B quantized models, which even in quantized form require 4-8 GB of RAM.\n",
        "\n",
        "Workaround: Install additional RAM modules. Using a large swap file (like yours uses 17.5 GB out of 21.4 GB) is an attempt to compensate for the lack of physical memory, but it is extremely slow for LLM.\n",
        "\n",
        "Free Disk Space: ✅ (153.81 GB). No upgrade required.\n",
        "\n",
        "OS (Linux/WSL2): ❌ (Windows 10 Home)\n",
        "\n",
        "Required update: For simplicity and to match typical instructions for running llama.cpp (and LLM in general) on a local machine without a GPU, you should install and configure WSL2 (Windows Subsystem for Linux 2) and install a Linux distribution (e.g. Ubuntu) on it. This will provide an environment where you can easily install the necessary tools (gcc, make, cmake) and compile llama.cpp.\n",
        "\n",
        "**Summary of required updates to run the 7B quantized model locally:**\n",
        "\n",
        "\"To successfully run the 7B quantized model locally, I first need to increase my RAM to 16GB or more. Additionally, to ensure compatibility and ease of tool deployment, I will need to install and configure WSL2 with a Linux distribution, as my Windows 10 Home operating system does not currently directly support optimal conditions for llama.cpp.\""
      ],
      "metadata": {
        "id": "64a7c9KYIghw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Benchmark-Based Model Explorer"
      ],
      "metadata": {
        "id": "U_9o65-vIyav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selected models:\n",
        "\n",
        "Mistral-7B-Instruct-v0.2: Often a strong all-rounder.\n",
        "\n",
        "Qwen1.5-7B-Chat: Known for its multilingualism and good overall performance.\n",
        "\n",
        "google/gemma-1.1-7b-it: A model from Google, interesting for comparison."
      ],
      "metadata": {
        "id": "FyEyTGhZKaS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record Benchmark Scores\n",
        "\n",
        "**Mistral-7B-Instruct-v0.2**\n",
        "\n",
        "HellaSwag Score: ~82.79 (based on some reports, the board itself may have a different average)\n",
        "\n",
        "MMLU Score: ~60.07\n",
        "\n",
        "**Qwen1.5-7B-Chat**\n",
        "\n",
        "HellaSwag Score: ~76.03\n",
        "\n",
        "MMLU Score: ~62.38\n",
        "\n",
        "**google/gemma-1.1-7b-it**\n",
        "\n",
        "HellaSwag Score: ~81.2\n",
        "\n",
        "MMLU Score: ~64.3"
      ],
      "metadata": {
        "id": "HWUN9mDkKgFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Check License Type\n",
        "\n",
        " Mistral-7B-Instruct-v0.2: Apache 2.0\n",
        "\n",
        "Qwen1.5-7B-Chat: Apache 2.0 (some variations of Qwen may use a different, but the base chat version is often under Apache 2.0 or similar permissive license)\n",
        "\n",
        "google/gemma-1.1-7b-it: Gemma License (Google specific license, which is permissive and allows commercial use)\n",
        "\n",
        "*Apache 2.0:* Very permissive license. Allows you to use, modify, distribute, sell, and sublicense the software without providing source code for derivative works. Excellent for commercial use.\n",
        "\n",
        "*Gemma License:* Also a permissive license from Google, allowing commercial use and modification, but with certain conditions regarding responsible use and compliance with their policies."
      ],
      "metadata": {
        "id": "KFkXuH4EKte1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Determine an Ideal Use Case"
      ],
      "metadata": {
        "id": "2kz7Df78LBUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mistral-7B-Instruct-v0.2**\n",
        "\n",
        "Ideal Use Case: Code generation, intelligent assistant for programmers, development of general-purpose multilingual chatbots. (High HellaSwag indicates good common sense, which is useful for a general assistant, and the Instruct version is good for following instructions).\n",
        "\n",
        "**Qwen1.5-7B-Chat**\n",
        "\n",
        "Ideal Use Case: Cross-cultural communication, multilingual translation and summarization, creation of educational materials (with a focus on various subject areas, thanks to MMLU).\n",
        "\n",
        "**google/gemma-1.1-7b-it**\n",
        "\n",
        "Ideal Use Case: Intelligent Q&A systems for a wide range of topics, assistants for academic and research tasks, content creation (thanks to the balance between common sense and academic knowledge)."
      ],
      "metadata": {
        "id": "ftYMnIG_LBzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Name\tHellaSwag Score\tMMLU Score\tLicense Type\tIdeal Use Case\n",
        "organization/model-name-A\t85.2\t54.3\tApache 2.0\tCommonsense Q&A agent-Code generation, intelligent assistant for programmers\n",
        "organization/model-name-B\t72.4\t62.1\tMIT\tAcademic question tutor-Multilingual assistant, translation and summarization\n",
        "organization/model-name-C\t80.0\t58.7\tCC BY 4.0\tMultilingual assistant-Gemma License Educational and research assistants"
      ],
      "metadata": {
        "id": "lzAOS2MVLO_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6: Cloud vs. Local Deployment Plan"
      ],
      "metadata": {
        "id": "2hZOVYeyLvB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost:** Initial cost, operating costs.\n",
        "\n",
        "**Latency/Performance:** Speed of response, availability of resources.\n",
        "\n",
        "**Control/Security:** Level of control over data and infrastructure.\n",
        "\n",
        "**Ease of Setup/Scalability:** How easy it is to get started and expand.\n",
        "\n",
        "**Maintenance:** The effort required to keep things running."
      ],
      "metadata": {
        "id": "zSE78ILNL07l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Identify Pros & Cons Categories**\n",
        "\n",
        "\n",
        "**Local Deployment**\n",
        "\n",
        "Advantages: Low latency and complete control over data (ideal for sensitive data).\n",
        "\n",
        "Disadvantages: On-premises: High upfront hardware costs (GPU, powerful PC) and difficulty scaling (limited to your physical machine).\n",
        "\n",
        "**Cloud Deployment**\n",
        "\n",
        "Advantages: Instant access to powerful GPUs and high on-demand scalability (quickly increase or decrease resources).\n",
        "Minimal equipment maintenance and ease of setup (no need for deep technical knowledge of the infrastructure).\n",
        "\n",
        "\n",
        "Disadvantages:Costs can add up quickly with heavy usage (hourly billing) and potential data security/privacy issues (data is held by a third party)."
      ],
      "metadata": {
        "id": "Cf5p40skMEkU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta2FPtBPRnCz",
        "outputId": "083aaa9f-a8f7-44c6-921d-ba7c554aa43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from huggingface_hub import login\n",
        "login(token=\"mytoken\")\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(model.device)\n",
        "start = time.time()\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "print(\"Elapsed:\", time.time() - start, \"seconds\")"
      ],
      "metadata": {
        "id": "eHlcb_T1Qv7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bKAhzhnCQvs6"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "HrIca1ykKrX2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlXNxCkRKm6s",
        "outputId": "ccf45539-cdba-435a-c1c6-b9cb2b761d90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7adffc9430>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "random_seed = 123\n",
        "\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Implementing the LoRALayer"
      ],
      "metadata": {
        "id": "D7G9nlOXKy0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        # Инициализируем матрицу A с нормальным распределением, масштабированным 1/sqrt(rank)\n",
        "        # Это помогает поддерживать норму активаций\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        # Инициализируем матрицу B нулями\n",
        "        # Это гарантирует, что в начале адаптация LoRA равна нулю,\n",
        "        # и модель начинает обучение с исходных весов.\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        # alpha - это коэффициент масштабирования для LoRA адаптации\n",
        "        # Он используется для управления вкладом LoRA в выходной сигнал.\n",
        "        self.alpha = alpha\n",
        "        # Ранг - это гиперпараметр, определяющий размер низкоранговых матриц.\n",
        "        # Более низкий ранг означает меньше параметров, но потенциально меньшую выразительность.\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Вычисляем LoRA трансформацию: x @ A @ B\n",
        "        # Затем масштабируем результат на (alpha / rank)\n",
        "        # Деление на rank используется для нормализации, чтобы избежать изменения масштаба при изменении rank.\n",
        "        x = (x @ self.A @ self.B) * (self.alpha / self.rank)\n",
        "        return x\n",
        "\n",
        "# Тестирование LoRALayer\n",
        "print(\"--- Exercise 1: LoRALayer ---\")\n",
        "in_features_test = 10\n",
        "out_features_test = 5\n",
        "rank_test = 4\n",
        "alpha_test = 8\n",
        "lora_layer_test = LoRALayer(in_features_test, out_features_test, rank_test, alpha_test)\n",
        "input_tensor_test = torch.randn(1, in_features_test) # Батч из 1 элемента\n",
        "output_lora_test = lora_layer_test(input_tensor_test)\n",
        "print(f\"LoRALayer Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LoRALayer Output Shape: {output_lora_test.shape}\")\n",
        "print(f\"LoRALayer Output (first 5 values): {output_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0EzuN8SK1f9",
        "outputId": "eb277043-f0a9-4432-bd8f-274624df13b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1: LoRALayer ---\n",
            "LoRALayer Input Shape: torch.Size([1, 10])\n",
            "LoRALayer Output Shape: torch.Size([1, 5])\n",
            "LoRALayer Output (first 5 values): tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Implementing the LinearWithLoRA Layer\n"
      ],
      "metadata": {
        "id": "3TevPMzZK2Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Extend a standard PyTorch Linear layer to incorporate the LoRALayer for adaptable training.\n",
        "\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        # Сохраняем исходный линейный слой\n",
        "        self.linear = linear\n",
        "        # Создаем экземпляр LoRALayer, используя in_features и out_features исходного линейного слоя\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Выходной сигнал - это сумма выхода исходного линейного слоя\n",
        "        # и выхода LoRA адаптации.\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "# Тестирование LinearWithLoRA\n",
        "print(\"--- Exercise 2: LinearWithLoRA ---\")\n",
        "linear_layer_orig = nn.Linear(in_features_test, out_features_test)\n",
        "linear_with_lora_test = LinearWithLoRA(linear_layer_orig, rank_test, alpha_test)\n",
        "output_linear_with_lora_test = linear_with_lora_test(input_tensor_test)\n",
        "print(f\"LinearWithLoRA Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output Shape: {output_linear_with_lora_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output (first 5 values): {output_linear_with_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0CF0_nYK7ZG",
        "outputId": "95ba1113-5c23-4069-f03a-78787b42781d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 2: LinearWithLoRA ---\n",
            "LinearWithLoRA Input Shape: torch.Size([1, 10])\n",
            "LinearWithLoRA Output Shape: torch.Size([1, 5])\n",
            "LinearWithLoRA Output (first 5 values): tensor([-0.3074,  0.4623, -0.6323,  0.1641,  0.1358], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Creating a Small Neural Network and Applying LoRA"
      ],
      "metadata": {
        "id": "PdxgpxfXLAd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Implement a simple feedforward neural network and apply LoRA to one of its layers.\n",
        "\n",
        "print(\"--- Exercise 3: Applying LoRA to a Single Layer ---\")\n",
        "# Определяем простой линейный слой\n",
        "layer = nn.Linear(in_features=10, out_features=5)\n",
        "# Генерируем случайный входной тензор\n",
        "x = torch.randn(1, 10)\n",
        "\n",
        "print(f\"Original Input: {x}\")\n",
        "print(f\"Original Linear Layer: {layer}\")\n",
        "original_output = layer(x)\n",
        "print('Original output:', original_output)\n",
        "\n",
        "# Применяем LoRA к линейному слою, заменяя его на LinearWithLoRA\n",
        "# Мы используем те же rank и alpha, что и ранее, или можем определить новые.\n",
        "# Здесь важно, что при инициализации LoRA.B нулями, начальный выход LoRA будет нулевым,\n",
        "# и, следовательно, выход LinearWithLoRA будет идентичен выходу оригинального Linear слоя.\n",
        "layer_lora_1 = LinearWithLoRA(layer, rank=4, alpha=8)\n",
        "lora_applied_output = layer_lora_1(x)\n",
        "print(f\"\\nLayer with LoRA Applied: {layer_lora_1}\")\n",
        "print('Output after applying LoRA (should be very close to original due to zero-initialized B):', lora_applied_output)\n",
        "\n",
        "# Проверяем, что выходы практически идентичны (из-за нулевой инициализации B в LoRALayer)\n",
        "print(f\"Difference between original and LoRA-applied output: {torch.sum(torch.abs(original_output - lora_applied_output))}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoVvU5OXLCrF",
        "outputId": "2ba9baea-00b9-4b4f-be17-5496c189b346"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 3: Applying LoRA to a Single Layer ---\n",
            "Original Input: tensor([[ 0.7934, -0.0819,  0.7044,  2.0753, -0.8251, -0.1351,  0.5037, -1.2158,\n",
            "          0.3821, -0.1739]])\n",
            "Original Linear Layer: Linear(in_features=10, out_features=5, bias=True)\n",
            "Original output: tensor([[ 0.6150, -0.0254, -0.1362,  1.0168, -0.1012]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Layer with LoRA Applied: LinearWithLoRA(\n",
            "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (lora): LoRALayer()\n",
            ")\n",
            "Output after applying LoRA (should be very close to original due to zero-initialized B): tensor([[ 0.6150, -0.0254, -0.1362,  1.0168, -0.1012]], grad_fn=<AddBackward0>)\n",
            "Difference between original and LoRA-applied output: 0.0\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Merging LoRA Matrices and Testing Equivalence"
      ],
      "metadata": {
        "id": "LdJIsGaXLG7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Implement an alternative approach where LoRA matrices are merged with the original weights for efficiency.\n",
        "\n",
        "class LinearWithLoRAMerged(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "        # Отключаем отслеживание градиентов для исходных весов linear слоя,\n",
        "        # так как мы будем изменять объединенные веса.\n",
        "        # Однако, для демонстрации эквивалентности, мы пока не замораживаем их здесь.\n",
        "        # Заморозка будет в Упражнении 6.\n",
        "        # self.linear.weight.requires_grad = False\n",
        "        # if self.linear.bias is not None:\n",
        "        #     self.linear.bias.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Объединяем матрицы LoRA: delta_W = alpha/rank * A @ B\n",
        "        lora_delta_weight = (self.lora.A @ self.lora.B).T * (self.lora.alpha / self.lora.rank)\n",
        "        # Затем объединяем LoRA адаптацию с исходными весами\n",
        "        # Важно: self.linear.weight - это (out_features, in_features)\n",
        "        # lora_delta_weight - это (out_features, in_features)\n",
        "        combined_weight = self.linear.weight + lora_delta_weight\n",
        "        # Используем F.linear для вычисления линейной трансформации с объединенными весами\n",
        "        return F.linear(x, combined_weight, self.linear.bias)\n",
        "\n",
        "print(\"--- Exercise 4: LinearWithLoRAMerged ---\")\n",
        "# Пересоздаем оригинальный линейный слой, чтобы его веса были нетронуты для сравнения\n",
        "layer_for_merge_test = nn.Linear(in_features=10, out_features=5)\n",
        "# Инициализируем LoRA merged слой, используя тот же исходный линейный слой\n",
        "layer_lora_2 = LinearWithLoRAMerged(layer_for_merge_test, rank=4, alpha=8)\n",
        "# Вычисляем выход с merged LoRA слоем\n",
        "merged_output = layer_lora_2(x)\n",
        "\n",
        "print(f\"Output from LinearWithLoRA (from Ex 3): {lora_applied_output}\")\n",
        "print(f\"Output from LinearWithLoRAMerged: {merged_output}\")\n",
        "# Проверяем эквивалентность\n",
        "print(f\"Difference between LinearWithLoRA and LinearWithLoRAMerged output: {torch.sum(torch.abs(lora_applied_output - merged_output))}\")\n",
        "print(\"As expected, the difference is negligible, demonstrating equivalence.\")\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPGy3m5SLKAl",
        "outputId": "9c47b995-580b-4e71-e8f5-199922ba1ebf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 4: LinearWithLoRAMerged ---\n",
            "Output from LinearWithLoRA (from Ex 3): tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddBackward0>)\n",
            "Output from LinearWithLoRAMerged: tensor([[ 0.2450,  0.1346, -0.1086, -0.6565, -0.0540]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Difference between LinearWithLoRA and LinearWithLoRAMerged output: 1.774471640586853\n",
            "As expected, the difference is negligible, demonstrating equivalence.\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Implementing a Multilayer Perceptron (MLP) and Replacing Layers with LoRA"
      ],
      "metadata": {
        "id": "ag1X0sWjLMrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Extend a simple MLP and modify its layers to use LoRA.\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes, use_lora=False, rank=4, alpha=8):\n",
        "        super().__init__()\n",
        "        # Сохраняем параметры для LoRA, если они используются\n",
        "        self.use_lora = use_lora\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Определяем слои MLP. Используем LinearWithLoRAMerged, если use_lora = True.\n",
        "        # Иначе используем стандартные nn.Linear.\n",
        "        if use_lora:\n",
        "            self.fc1 = LinearWithLoRAMerged(nn.Linear(num_features, num_hidden_1), rank=rank, alpha=alpha)\n",
        "            self.fc2 = LinearWithLoRAMerged(nn.Linear(num_hidden_1, num_hidden_2), rank=rank, alpha=alpha)\n",
        "            self.fc3 = LinearWithLoRAMerged(nn.Linear(num_hidden_2, num_classes), rank=rank, alpha=alpha)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(num_features, num_hidden_1)\n",
        "            self.fc2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
        "            self.fc3 = nn.Linear(num_hidden_2, num_classes)\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "          self.fc1,\n",
        "          nn.ReLU(),\n",
        "          self.fc2,\n",
        "          nn.ReLU(),\n",
        "          self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Перед тем как передать в слои, вытягиваем входной тензор (flatten)\n",
        "        # Это типично для MLP при работе с изображениями, например MNIST.\n",
        "        x = x.view(x.size(0), -1) # Flatten the input\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "print(\"--- Exercise 5: MLP with LoRA Layers ---\")\n",
        "# Architecture (для MNIST)\n",
        "num_features = 28*28 # Размер изображения MNIST: 28x28\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 64\n",
        "num_classes = 10 # 10 цифр\n",
        "\n",
        "# Settings\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10 # Уменьшено для более быстрого выполнения примера\n",
        "\n",
        "# Создаем модель MLP с LoRA\n",
        "model = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=True, # Включаем LoRA для всех слоев\n",
        "    rank=4, # Пример ранга\n",
        "    alpha=8 # Пример alpha\n",
        ")\n",
        "\n",
        "model.to(DEVICE)\n",
        "optimizer_pretrained = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(\"Model Architecture (with LoRA Merged Layers):\")\n",
        "print(model)\n",
        "print(f\"Optimizer: {optimizer_pretrained}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "# Loading dataset\n",
        "BATCH_SIZE = 64\n",
        "# Note: transforms.ToTensor() scales input images to 0-1 range\n",
        "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Проверка размерностей батча\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape) # Ожидается: torch.Size([64, 1, 28, 28])\n",
        "    print('Image label dimensions:', labels.shape) # Ожидается: torch.Size([64])\n",
        "    break\n",
        "\n",
        "# Define evaluation\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "        return correct_pred.float() / num_examples * 100\n",
        "\n",
        "# Training (используем функцию train для оригинальной модели, чтобы получить базовую производительность)\n",
        "def train(num_epochs, model, optimizer, train_loader, device):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # forward and back propagation\n",
        "            logits = model(features)\n",
        "            loss = F.cross_entropy(logits, targets) # Используем CrossEntropyLoss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # logging\n",
        "            if not batch_idx % 400: # Логируем каждые 400 батчей\n",
        "                print('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' % (\n",
        "                    epoch + 1, num_epochs, batch_idx, len(train_loader), loss.item()))\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            train_acc = compute_accuracy(model, train_loader, device)\n",
        "            print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch + 1, num_epochs, train_acc))\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))\n",
        "\n",
        "print(\"--- Initial Training of MLP with LoRA Merged Layers (as per Ex 5 setup) ---\")\n",
        "# Тренируем модель, созданную в упражнении 5, которая уже использует LinearWithLoRAMerged\n",
        "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
        "print(f'Test accuracy after initial training: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "\n",
        "# Replacing Linear with LoRA Layers (This part is conceptually handled by use_lora=True in MLP)\n",
        "# The provided template suggests deepcopying and then replacing layers.\n",
        "# Let's create a \"base\" model first without LoRA for comparison.\n",
        "model_base = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=False # Это будет наш базовый MLP без LoRA\n",
        ")\n",
        "model_base.to(DEVICE)\n",
        "# Тренируем базовую модель для сравнения производительности, если хотите.\n",
        "# train(num_epochs, model_base, torch.optim.Adam(model_base.parameters(), lr=learning_rate), train_loader, DEVICE)\n",
        "\n",
        "print(\"--- Replacing Layers with LoRA (demonstration of replacement on a base model) ---\")\n",
        "# Создаем копию базовой модели для применения LoRA\n",
        "model_lora = copy.deepcopy(model_base)\n",
        "\n",
        "# Заменяем каждый Linear слой на LinearWithLoRAMerged\n",
        "model_lora.fc1 = LinearWithLoRAMerged(model_lora.fc1, rank=4, alpha=8)\n",
        "model_lora.fc2 = LinearWithLoRAMerged(model_lora.fc2, rank=4, alpha=8) # Замена fc2\n",
        "model_lora.fc3 = LinearWithLoRAMerged(model_lora.fc3, rank=4, alpha=8) # Замена fc3\n",
        "\n",
        "# Обновляем nn.Sequential, чтобы он использовал новые слои с LoRA\n",
        "# Это важно, так как nn.Sequential хранит ссылки на объекты слоев.\n",
        "# В нашей реализации MultilayerPerceptron, если use_lora=True, это происходит автоматически.\n",
        "# Но если мы делаем это вручную через deepcopy и замену, нам нужно обновить Sequential.\n",
        "# Однако, более простой способ, как показано в MultilayerPerceptron выше, это создавать LoRA слои сразу.\n",
        "# Если вы используете предоставленный шаблон, то вам нужно будет вручную заменить слои в `model_lora.layers`.\n",
        "# Поскольку MultilayerPerceptron уже умеет создавать слои с LoRA, эта часть кода может быть переосмыслена.\n",
        "# Для целей демонстрации шаблона:\n",
        "# model_lora.layers[0] = LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
        "# model_lora.layers[2] = LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
        "# model_lora.layers[4] = LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
        "# Но так как мы создали model_lora с use_lora=True, то вышеуказанное не нужно.\n",
        "# Для ясности, давайте создадим 'model_lora' снова, чтобы убедиться, что оно соответствует.\n",
        "# model_lora = MultilayerPerceptron(num_features, num_hidden_1, num_hidden_2, num_classes, use_lora=True, rank=4, alpha=8)\n",
        "# model_lora.to(DEVICE)\n",
        "\n",
        "\n",
        "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "print(\"Model Architecture After Manual LoRA Replacement (example):\")\n",
        "print(model_lora)\n",
        "\n",
        "print(f'\\nTest accuracy original model (if trained): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model (before specific LoRA training): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkxl8yKnLah9",
        "outputId": "5eca8637-ab57-4469-d5bc-18e2e2538f8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 5: MLP with LoRA Layers ---\n",
            "Device: cuda\n",
            "Model Architecture (with LoRA Merged Layers):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc2): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc3): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (1): ReLU()\n",
            "    (2): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (3): ReLU()\n",
            "    (4): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "----------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.51MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 65.2kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.03MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([64])\n",
            "--- Initial Training of MLP with LoRA Merged Layers (as per Ex 5 setup) ---\n",
            "Epoch: 001/010 | Batch 000/938 | Loss: 2.3219\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.1071\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.1535\n",
            "Epoch: 001/010 training accuracy: 95.79%\n",
            "Time elapsed: 0.25 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.1328\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.0585\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.1594\n",
            "Epoch: 002/010 training accuracy: 97.28%\n",
            "Time elapsed: 0.49 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0750\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0132\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.0443\n",
            "Epoch: 003/010 training accuracy: 98.01%\n",
            "Time elapsed: 0.72 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0394\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.0208\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0461\n",
            "Epoch: 004/010 training accuracy: 98.39%\n",
            "Time elapsed: 0.94 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0236\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0295\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.0535\n",
            "Epoch: 005/010 training accuracy: 98.71%\n",
            "Time elapsed: 1.17 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0159\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0095\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0262\n",
            "Epoch: 006/010 training accuracy: 98.81%\n",
            "Time elapsed: 1.41 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0448\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0822\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0434\n",
            "Epoch: 007/010 training accuracy: 99.18%\n",
            "Time elapsed: 1.67 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0172\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0305\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0321\n",
            "Epoch: 008/010 training accuracy: 99.07%\n",
            "Time elapsed: 1.89 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0296\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0204\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0110\n",
            "Epoch: 009/010 training accuracy: 99.33%\n",
            "Time elapsed: 2.12 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0256\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0049\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0079\n",
            "Epoch: 010/010 training accuracy: 99.51%\n",
            "Time elapsed: 2.35 min\n",
            "Total Training Time: 2.35 min\n",
            "Test accuracy after initial training: 97.91%\n",
            "----------------------------\n",
            "\n",
            "--- Replacing Layers with LoRA (demonstration of replacement on a base model) ---\n",
            "Model Architecture After Manual LoRA Replacement (example):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc2): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc3): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test accuracy original model (if trained): 8.87%\n",
            "Test accuracy LoRA model (before specific LoRA training): 8.87%\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌟Exercise 6: Freezing the Original Linear Layers and Training LoRA"
      ],
      "metadata": {
        "id": "zi17cBOjLhyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 🌟 Exercise 6: Заморозка оригинальных линейных слоев и обучение LoRA ---\n",
        "print(\"--- Exercise 6: Freezing Original Linear Layers ---\")\n",
        "\n",
        "def freeze_linear_layers(model):\n",
        "    # Используем named_modules для обхода всех подмодулей, включая вложенные\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LinearWithLoRAMerged):\n",
        "            # Если это наш LoRA-обернутый слой, замораживаем его внутренний 'linear' слой\n",
        "            for param in module.linear.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            # Это может быть полезно, если в модели есть стандартные Linear слои,\n",
        "            # которые не обернуты LoRA, и вы хотите их заморозить.\n",
        "            # В нашем MLP с use_lora=True все Linear слои обернуты.\n",
        "            # Но если use_lora=False, то это сработает для model_base.\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "# Применяем функцию заморозки к нашей модели с LoRA\n",
        "freeze_linear_layers(model_lora)\n",
        "\n",
        "print(\"\\nTrainable parameters after freezing:\")\n",
        "trainable_params_exist = False\n",
        "for name, param in model_lora.named_parameters():\n",
        "    print(f'{name}: {param.requires_grad}')\n",
        "    if param.requires_grad:\n",
        "        trainable_params_exist = True\n",
        "if not trainable_params_exist:\n",
        "    print(\"No trainable parameters found. Something might be wrong with freezing logic or model structure.\")\n",
        "else:\n",
        "    print(\"\\nConfirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\")\n",
        "\n",
        "# Создаем новый оптимизатор, который будет оптимизировать только обучаемые параметры\n",
        "# Это критический шаг: оптимизатор должен видеть только те параметры, которые имеют requires_grad=True\n",
        "optimizer_lora_finetune = torch.optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=learning_rate)\n",
        "print(f\"\\nOptimizer for fine-tuning LoRA: {optimizer_lora_finetune}\")\n",
        "\n",
        "print(\"\\n--- Training LoRA-tuned Model ---\")\n",
        "# Тренируем модель с замороженными оригинальными слоями, обучаются только LoRA адаптеры\n",
        "train(num_epochs, model_lora, optimizer_lora_finetune, train_loader, DEVICE)\n",
        "print(f'\\nTest accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "# Снова проверяем производительность для сравнения\n",
        "print(f'\\nTest accuracy original MLP (model_base, if trained initially): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model (after finetuning): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "TrcaOfLiLlJV",
        "outputId": "949fd0db-a997-4c1b-a264-a1052cfa7eaa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 6: Freezing Original Linear Layers ---\n",
            "\n",
            "Trainable parameters after freezing:\n",
            "fc1.linear.weight: False\n",
            "fc1.linear.bias: False\n",
            "fc1.lora.A: True\n",
            "fc1.lora.B: True\n",
            "fc2.linear.weight: False\n",
            "fc2.linear.bias: False\n",
            "fc2.lora.A: True\n",
            "fc2.lora.B: True\n",
            "fc3.linear.weight: False\n",
            "fc3.linear.bias: False\n",
            "fc3.lora.A: True\n",
            "fc3.lora.B: True\n",
            "\n",
            "Confirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\n",
            "\n",
            "Optimizer for fine-tuning LoRA: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "--- Training LoRA-tuned Model ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-2085058497.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Training LoRA-tuned Model ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Тренируем модель с замороженными оригинальными слоями, обучаются только LoRA адаптеры\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_lora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lora_finetune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nTest accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-1072104938.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, optimizer, train_loader, device)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Гиперпараметры ---\n",
        "random_seed = 123\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Архитектура (для MNIST)\n",
        "num_features = 28 * 28  # Размер изображения MNIST: 28x28\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 64\n",
        "num_classes = 10  # 10 цифр\n",
        "\n",
        "# Настройки обучения\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10  # Уменьшено для более быстрого выполнения примера\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# --- 🌟 Exercise 1: Реализация LoRALayer ---\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        # Инициализируем матрицу A из нормального распределения, масштабированного 1/sqrt(rank)\n",
        "        # Это помогает поддерживать норму активаций.\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        # Инициализируем матрицу B нулями. Это гарантирует, что в начале адаптация LoRA\n",
        "        # не изменяет выходной сигнал, и модель начинает обучение с исходных весов.\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        # Коэффициент масштабирования для LoRA адаптации.\n",
        "        # Деление на rank используется для нормализации.\n",
        "        self.alpha = alpha\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Вычисляем LoRA трансформацию: x @ A @ B\n",
        "        # Затем масштабируем результат на (alpha / rank)\n",
        "        x = (x @ self.A @ self.B) * (self.alpha / self.rank)\n",
        "        return x\n",
        "\n",
        "# --- 🌟 Exercise 2: Реализация LinearWithLoRA Layer ---\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        # Сохраняем ссылку на исходный nn.Linear слой\n",
        "        self.linear = linear\n",
        "        # Создаем экземпляр LoRALayer\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Выходной сигнал - это сумма выхода исходного линейного слоя\n",
        "        # и выхода LoRA адаптации.\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "# --- 🌟 Exercise 4: Реализация LinearWithLoRAMerged Layer ---\n",
        "# (Упражнение 3 тестируется после создания MLP)\n",
        "class LinearWithLoRAMerged(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Вычисляем дельта-веса от LoRA: delta_W = alpha/rank * A @ B\n",
        "        # .T используется, потому что PyTorch хранит веса как (out_features, in_features)\n",
        "        lora_delta_weight = (self.lora.A @ self.lora.B).T * (self.lora.alpha / self.lora.rank)\n",
        "        # Объединяем LoRA адаптацию с исходными весами\n",
        "        combined_weight = self.linear.weight + lora_delta_weight\n",
        "        # Используем F.linear для вычисления линейной трансформации\n",
        "        return F.linear(x, combined_weight, self.linear.bias)\n",
        "\n",
        "# --- 🌟 Exercise 5: Реализация Multilayer Perceptron (MLP) с опцией LoRA ---\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes, use_lora=False, rank=4, alpha=8):\n",
        "        super().__init__()\n",
        "        self.use_lora = use_lora\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Определяем слои MLP. Используем LinearWithLoRAMerged, если use_lora = True,\n",
        "        # иначе используем стандартные nn.Linear.\n",
        "        if use_lora:\n",
        "            self.fc1 = LinearWithLoRAMerged(nn.Linear(num_features, num_hidden_1), rank=rank, alpha=alpha)\n",
        "            self.fc2 = LinearWithLoRAMerged(nn.Linear(num_hidden_1, num_hidden_2), rank=rank, alpha=alpha)\n",
        "            self.fc3 = LinearWithLoRAMerged(nn.Linear(num_hidden_2, num_classes), rank=rank, alpha=alpha)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(num_features, num_hidden_1)\n",
        "            self.fc2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
        "            self.fc3 = nn.Linear(num_hidden_2, num_classes)\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            self.fc1,\n",
        "            nn.ReLU(),\n",
        "            self.fc2,\n",
        "            nn.ReLU(),\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Вытягиваем входной тензор (flatten) для MLP\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "# --- Загрузка набора данных ---\n",
        "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# --- Вспомогательная функция для вычисления точности ---\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "        return correct_pred.float() / num_examples * 100\n",
        "\n",
        "# --- Вспомогательная функция для тренировки модели ---\n",
        "def train(num_epochs, model, optimizer, train_loader, device):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Прямое и обратное распространение\n",
        "            logits = model(features)\n",
        "            loss = F.cross_entropy(logits, targets) # Используем CrossEntropyLoss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Логирование\n",
        "            if not batch_idx % 400:\n",
        "                print('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' % (\n",
        "                    epoch + 1, num_epochs, batch_idx, len(train_loader), loss.item()))\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            train_acc = compute_accuracy(model, train_loader, device)\n",
        "            print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch + 1, num_epochs, train_acc))\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))\n",
        "\n",
        "\n",
        "# --- Демонстрация работы упражнений ---\n",
        "\n",
        "print(\"--- Exercise 1: LoRALayer ---\")\n",
        "in_features_test = 10\n",
        "out_features_test = 5\n",
        "rank_test = 4\n",
        "alpha_test = 8\n",
        "lora_layer_test = LoRALayer(in_features_test, out_features_test, rank_test, alpha_test)\n",
        "input_tensor_test = torch.randn(1, in_features_test)\n",
        "output_lora_test = lora_layer_test(input_tensor_test)\n",
        "print(f\"LoRALayer Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LoRALayer Output Shape: {output_lora_test.shape}\")\n",
        "print(f\"LoRALayer Output (first 5 values): {output_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 2: LinearWithLoRA ---\")\n",
        "linear_layer_orig = nn.Linear(in_features_test, out_features_test)\n",
        "linear_with_lora_test = LinearWithLoRA(linear_layer_orig, rank_test, alpha_test)\n",
        "output_linear_with_lora_test = linear_with_lora_test(input_tensor_test)\n",
        "print(f\"LinearWithLoRA Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output Shape: {output_linear_with_lora_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output (first 5 values): {output_linear_with_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 3: Создание небольшой нейронной сети и применение LoRA ---\")\n",
        "layer_ex3 = nn.Linear(in_features=10, out_features=5)\n",
        "x_ex3 = torch.randn(1, 10)\n",
        "\n",
        "print(f\"Original Input: {x_ex3}\")\n",
        "print(f\"Original Linear Layer: {layer_ex3}\")\n",
        "original_output_ex3 = layer_ex3(x_ex3)\n",
        "print('Original output:', original_output_ex3)\n",
        "\n",
        "layer_lora_1_ex3 = LinearWithLoRA(layer_ex3, rank=4, alpha=8)\n",
        "lora_applied_output_ex3 = layer_lora_1_ex3(x_ex3)\n",
        "print(f\"\\nLayer with LoRA Applied: {layer_lora_1_ex3}\")\n",
        "print('Output after applying LoRA (should be very close to original due to zero-initialized B):', lora_applied_output_ex3)\n",
        "print(f\"Difference between original and LoRA-applied output: {torch.sum(torch.abs(original_output_ex3 - lora_applied_output_ex3))}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 4: Merging LoRA Matrices and Testing Equivalence ---\")\n",
        "layer_for_merge_test = nn.Linear(in_features=10, out_features=5)\n",
        "# Используем те же веса для LinearWithLoRA для корректного сравнения\n",
        "layer_for_merge_test.load_state_dict(layer_ex3.state_dict())\n",
        "\n",
        "layer_lora_2_ex4 = LinearWithLoRAMerged(layer_for_merge_test, rank=4, alpha=8)\n",
        "merged_output_ex4 = layer_lora_2_ex4(x_ex3)\n",
        "\n",
        "print(f\"Output from LinearWithLoRA (from Ex 3): {lora_applied_output_ex3}\")\n",
        "print(f\"Output from LinearWithLoRAMerged: {merged_output_ex4}\")\n",
        "print(f\"Difference between LinearWithLoRA and LinearWithLoRAMerged output: {torch.sum(torch.abs(lora_applied_output_ex3 - merged_output_ex4))}\")\n",
        "print(\"As expected, the difference is negligible, demonstrating equivalence.\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 5: Реализация Multilayer Perceptron (MLP) и замена слоев на LoRA ---\")\n",
        "# Создаем базовую модель без LoRA для сравнения (чтобы потом на нее можно было наложить LoRA или сравнить)\n",
        "model_base = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=False # Базовая модель без LoRA\n",
        ")\n",
        "model_base.to(DEVICE)\n",
        "print(\"Model Architecture (Base MLP without LoRA):\")\n",
        "print(model_base)\n",
        "print(f'\\nTest accuracy original MLP (before any training): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "\n",
        "# Создаем модель MLP, которая уже использует LinearWithLoRAMerged\n",
        "model_lora = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=True, # Включаем LoRA для всех слоев\n",
        "    rank=4,\n",
        "    alpha=8\n",
        ")\n",
        "model_lora.to(DEVICE)\n",
        "optimizer_initial_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "print(\"\\nModel Architecture (MLP with LoRA Merged Layers - initial setup):\")\n",
        "print(model_lora)\n",
        "print(f'\\nTest accuracy LoRA model (before initial training): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "\n",
        "print(\"\\n--- Initial Training of MLP with LoRA Merged Layers (Ex 5 setup) ---\")\n",
        "# Тренируем модель LoRA, которая изначально имеет все параметры обучаемыми\n",
        "# Это даст нам базовую производительность модели с LoRA до \"тонкой настройки\"\n",
        "train(num_epochs, model_lora, optimizer_initial_lora, train_loader, DEVICE)\n",
        "print(f'\\nTest accuracy after initial training of LoRA MLP: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "\n",
        "# --- 🌟 Exercise 6: Заморозка оригинальных линейных слоев и обучение LoRA ---\n",
        "print(\"--- Exercise 6: Freezing Original Linear Layers ---\")\n",
        "\n",
        "def freeze_linear_layers(model):\n",
        "    # Используем named_modules для обхода всех подмодулей, включая вложенные\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LinearWithLoRAMerged):\n",
        "            # Если это наш LoRA-обернутый слой, замораживаем его внутренний 'linear' слой\n",
        "            for param in module.linear.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            # Это может быть полезно, если в модели есть стандартные Linear слои,\n",
        "            # которые не обернуты LoRA, и вы хотите их заморозить.\n",
        "            # В нашем MLP с use_lora=True все Linear слои обернуты.\n",
        "            # Но если use_lora=False, то это сработает для model_base.\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "# Применяем функцию заморозки к нашей модели с LoRA\n",
        "freeze_linear_layers(model_lora)\n",
        "\n",
        "print(\"\\nTrainable parameters after freezing:\")\n",
        "trainable_params_exist = False\n",
        "for name, param in model_lora.named_parameters():\n",
        "    print(f'{name}: {param.requires_grad}')\n",
        "    if param.requires_grad:\n",
        "        trainable_params_exist = True\n",
        "if not trainable_params_exist:\n",
        "    print(\"No trainable parameters found. Something might be wrong with freezing logic or model structure.\")\n",
        "else:\n",
        "    print(\"\\nConfirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\")\n",
        "\n",
        "# Создаем новый оптимизатор, который будет оптимизировать только обучаемые параметры\n",
        "# Это критический шаг: оптимизатор должен видеть только те параметры, которые имеют requires_grad=True\n",
        "optimizer_lora_finetune = torch.optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=learning_rate)\n",
        "print(f\"\\nOptimizer for fine-tuning LoRA: {optimizer_lora_finetune}\")\n",
        "\n",
        "print(\"\\n--- Training LoRA-tuned Model ---\")\n",
        "# Тренируем модель с замороженными оригинальными слоями, обучаются только LoRA адаптеры\n",
        "train(num_epochs, model_lora, optimizer_lora_finetune, train_loader, DEVICE)\n",
        "print(f'\\nTest accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "# Снова проверяем производительность для сравнения\n",
        "print(f'\\nTest accuracy original MLP (model_base, if trained initially): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model (after finetuning): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LOgoLXmS5bI",
        "outputId": "be5ed699-622e-4e1e-c368-db3d4c35df84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1: LoRALayer ---\n",
            "LoRALayer Input Shape: torch.Size([1, 10])\n",
            "LoRALayer Output Shape: torch.Size([1, 5])\n",
            "LoRALayer Output (first 5 values): tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 2: LinearWithLoRA ---\n",
            "LinearWithLoRA Input Shape: torch.Size([1, 10])\n",
            "LinearWithLoRA Output Shape: torch.Size([1, 5])\n",
            "LinearWithLoRA Output (first 5 values): tensor([-0.3074,  0.4623, -0.6323,  0.1641,  0.1358], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 3: Создание небольшой нейронной сети и применение LoRA ---\n",
            "Original Input: tensor([[ 0.0142,  0.1918,  0.4896, -0.0594, -1.0748,  0.1630,  0.5262, -1.3971,\n",
            "         -0.3554, -0.6451]])\n",
            "Original Linear Layer: Linear(in_features=10, out_features=5, bias=True)\n",
            "Original output: tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Layer with LoRA Applied: LinearWithLoRA(\n",
            "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (lora): LoRALayer()\n",
            ")\n",
            "Output after applying LoRA (should be very close to original due to zero-initialized B): tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddBackward0>)\n",
            "Difference between original and LoRA-applied output: 0.0\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 4: Merging LoRA Matrices and Testing Equivalence ---\n",
            "Output from LinearWithLoRA (from Ex 3): tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddBackward0>)\n",
            "Output from LinearWithLoRAMerged: tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddmmBackward0>)\n",
            "Difference between LinearWithLoRA and LinearWithLoRAMerged output: 0.0\n",
            "As expected, the difference is negligible, demonstrating equivalence.\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 5: Реализация Multilayer Perceptron (MLP) и замена слоев на LoRA ---\n",
            "Model Architecture (Base MLP without LoRA):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test accuracy original MLP (before any training): 9.32%\n",
            "\n",
            "Model Architecture (MLP with LoRA Merged Layers - initial setup):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc2): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc3): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (1): ReLU()\n",
            "    (2): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (3): ReLU()\n",
            "    (4): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "Test accuracy LoRA model (before initial training): 6.03%\n",
            "\n",
            "--- Initial Training of MLP with LoRA Merged Layers (Ex 5 setup) ---\n",
            "Epoch: 001/010 | Batch 000/938 | Loss: 2.2977\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.2227\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.3254\n",
            "Epoch: 001/010 training accuracy: 95.71%\n",
            "Time elapsed: 0.22 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.1192\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.1035\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.3081\n",
            "Epoch: 002/010 training accuracy: 97.06%\n",
            "Time elapsed: 0.45 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0919\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0620\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.1806\n",
            "Epoch: 003/010 training accuracy: 97.77%\n",
            "Time elapsed: 0.67 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0696\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.1198\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0086\n",
            "Epoch: 004/010 training accuracy: 98.04%\n",
            "Time elapsed: 0.89 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0936\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0082\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.1061\n",
            "Epoch: 005/010 training accuracy: 98.50%\n",
            "Time elapsed: 1.12 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0041\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0146\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0233\n",
            "Epoch: 006/010 training accuracy: 98.72%\n",
            "Time elapsed: 1.35 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0248\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0043\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0119\n",
            "Epoch: 007/010 training accuracy: 98.66%\n",
            "Time elapsed: 1.58 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0746\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0068\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0129\n",
            "Epoch: 008/010 training accuracy: 98.94%\n",
            "Time elapsed: 1.81 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0102\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0032\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0064\n",
            "Epoch: 009/010 training accuracy: 99.44%\n",
            "Time elapsed: 2.03 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0072\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0008\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0086\n",
            "Epoch: 010/010 training accuracy: 99.43%\n",
            "Time elapsed: 2.25 min\n",
            "Total Training Time: 2.25 min\n",
            "\n",
            "Test accuracy after initial training of LoRA MLP: 97.82%\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 6: Freezing Original Linear Layers ---\n",
            "\n",
            "Trainable parameters after freezing:\n",
            "fc1.linear.weight: False\n",
            "fc1.linear.bias: False\n",
            "fc1.lora.A: True\n",
            "fc1.lora.B: True\n",
            "fc2.linear.weight: False\n",
            "fc2.linear.bias: False\n",
            "fc2.lora.A: True\n",
            "fc2.lora.B: True\n",
            "fc3.linear.weight: False\n",
            "fc3.linear.bias: False\n",
            "fc3.lora.A: True\n",
            "fc3.lora.B: True\n",
            "\n",
            "Confirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\n",
            "\n",
            "Optimizer for fine-tuning LoRA: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "--- Training LoRA-tuned Model ---\n",
            "Epoch: 001/010 | Batch 000/938 | Loss: 0.0091\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.0173\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.0227\n",
            "Epoch: 001/010 training accuracy: 99.79%\n",
            "Time elapsed: 0.22 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.0251\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.0022\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.0071\n",
            "Epoch: 002/010 training accuracy: 99.85%\n",
            "Time elapsed: 0.44 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0104\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0239\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.0042\n",
            "Epoch: 003/010 training accuracy: 99.88%\n",
            "Time elapsed: 0.65 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0034\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.0188\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0034\n",
            "Epoch: 004/010 training accuracy: 99.84%\n",
            "Time elapsed: 0.87 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0004\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0013\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.0022\n",
            "Epoch: 005/010 training accuracy: 99.88%\n",
            "Time elapsed: 1.09 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0050\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0010\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0084\n",
            "Epoch: 006/010 training accuracy: 99.90%\n",
            "Time elapsed: 1.31 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0006\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0042\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0003\n",
            "Epoch: 007/010 training accuracy: 99.90%\n",
            "Time elapsed: 1.54 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0005\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0005\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0054\n",
            "Epoch: 008/010 training accuracy: 99.88%\n",
            "Time elapsed: 1.77 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0047\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0022\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0029\n",
            "Epoch: 009/010 training accuracy: 99.84%\n",
            "Time elapsed: 1.99 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0011\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0102\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0017\n",
            "Epoch: 010/010 training accuracy: 99.91%\n",
            "Time elapsed: 2.21 min\n",
            "Total Training Time: 2.21 min\n",
            "\n",
            "Test accuracy LoRA finetune: 97.85%\n",
            "\n",
            "Test accuracy original MLP (model_base, if trained initially): 9.32%\n",
            "Test accuracy LoRA model (after finetuning): 97.85%\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
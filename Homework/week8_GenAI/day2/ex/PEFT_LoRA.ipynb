{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "HrIca1ykKrX2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlXNxCkRKm6s",
        "outputId": "ccf45539-cdba-435a-c1c6-b9cb2b761d90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7adffc9430>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "random_seed = 123\n",
        "\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Implementing the LoRALayer"
      ],
      "metadata": {
        "id": "D7G9nlOXKy0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É A —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–º 1/sqrt(rank)\n",
        "        # –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –Ω–æ—Ä–º—É –∞–∫—Ç–∏–≤–∞—Ü–∏–π\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É B –Ω—É–ª—è–º–∏\n",
        "        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤ –Ω–∞—á–∞–ª–µ –∞–¥–∞–ø—Ç–∞—Ü–∏—è LoRA —Ä–∞–≤–Ω–∞ –Ω—É–ª—é,\n",
        "        # –∏ –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å—Ö–æ–¥–Ω—ã—Ö –≤–µ—Å–æ–≤.\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        # alpha - —ç—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è LoRA –∞–¥–∞–ø—Ç–∞—Ü–∏–∏\n",
        "        # –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∫–ª–∞–¥–æ–º LoRA –≤ –≤—ã—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª.\n",
        "        self.alpha = alpha\n",
        "        # –†–∞–Ω–≥ - —ç—Ç–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π —Ä–∞–∑–º–µ—Ä –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã—Ö –º–∞—Ç—Ä–∏—Ü.\n",
        "        # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π —Ä–∞–Ω–≥ –æ–∑–Ω–∞—á–∞–µ—Ç –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–µ–Ω—å—à—É—é –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º LoRA —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é: x @ A @ B\n",
        "        # –ó–∞—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ (alpha / rank)\n",
        "        # –î–µ–ª–µ–Ω–∏–µ –Ω–∞ rank –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∞ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ rank.\n",
        "        x = (x @ self.A @ self.B) * (self.alpha / self.rank)\n",
        "        return x\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LoRALayer\n",
        "print(\"--- Exercise 1: LoRALayer ---\")\n",
        "in_features_test = 10\n",
        "out_features_test = 5\n",
        "rank_test = 4\n",
        "alpha_test = 8\n",
        "lora_layer_test = LoRALayer(in_features_test, out_features_test, rank_test, alpha_test)\n",
        "input_tensor_test = torch.randn(1, in_features_test) # –ë–∞—Ç—á –∏–∑ 1 —ç–ª–µ–º–µ–Ω—Ç–∞\n",
        "output_lora_test = lora_layer_test(input_tensor_test)\n",
        "print(f\"LoRALayer Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LoRALayer Output Shape: {output_lora_test.shape}\")\n",
        "print(f\"LoRALayer Output (first 5 values): {output_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0EzuN8SK1f9",
        "outputId": "eb277043-f0a9-4432-bd8f-274624df13b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1: LoRALayer ---\n",
            "LoRALayer Input Shape: torch.Size([1, 10])\n",
            "LoRALayer Output Shape: torch.Size([1, 5])\n",
            "LoRALayer Output (first 5 values): tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Implementing the LinearWithLoRA Layer\n"
      ],
      "metadata": {
        "id": "3TevPMzZK2Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Extend a standard PyTorch Linear layer to incorporate the LoRALayer for adaptable training.\n",
        "\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
        "        self.linear = linear\n",
        "        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä LoRALayer, –∏—Å–ø–æ–ª—å–∑—É—è in_features –∏ out_features –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª - —ç—Ç–æ —Å—É–º–º–∞ –≤—ã—Ö–æ–¥–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –∏ –≤—ã—Ö–æ–¥–∞ LoRA –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LinearWithLoRA\n",
        "print(\"--- Exercise 2: LinearWithLoRA ---\")\n",
        "linear_layer_orig = nn.Linear(in_features_test, out_features_test)\n",
        "linear_with_lora_test = LinearWithLoRA(linear_layer_orig, rank_test, alpha_test)\n",
        "output_linear_with_lora_test = linear_with_lora_test(input_tensor_test)\n",
        "print(f\"LinearWithLoRA Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output Shape: {output_linear_with_lora_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output (first 5 values): {output_linear_with_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0CF0_nYK7ZG",
        "outputId": "95ba1113-5c23-4069-f03a-78787b42781d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 2: LinearWithLoRA ---\n",
            "LinearWithLoRA Input Shape: torch.Size([1, 10])\n",
            "LinearWithLoRA Output Shape: torch.Size([1, 5])\n",
            "LinearWithLoRA Output (first 5 values): tensor([-0.3074,  0.4623, -0.6323,  0.1641,  0.1358], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Creating a Small Neural Network and Applying LoRA"
      ],
      "metadata": {
        "id": "PdxgpxfXLAd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Implement a simple feedforward neural network and apply LoRA to one of its layers.\n",
        "\n",
        "print(\"--- Exercise 3: Applying LoRA to a Single Layer ---\")\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
        "layer = nn.Linear(in_features=10, out_features=5)\n",
        "# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä\n",
        "x = torch.randn(1, 10)\n",
        "\n",
        "print(f\"Original Input: {x}\")\n",
        "print(f\"Original Linear Layer: {layer}\")\n",
        "original_output = layer(x)\n",
        "print('Original output:', original_output)\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –∫ –ª–∏–Ω–µ–π–Ω–æ–º—É —Å–ª–æ—é, –∑–∞–º–µ–Ω—è—è –µ–≥–æ –Ω–∞ LinearWithLoRA\n",
        "# –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ rank –∏ alpha, —á—Ç–æ –∏ —Ä–∞–Ω–µ–µ, –∏–ª–∏ –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–æ–≤—ã–µ.\n",
        "# –ó–¥–µ—Å—å –≤–∞–∂–Ω–æ, —á—Ç–æ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LoRA.B –Ω—É–ª—è–º–∏, –Ω–∞—á–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ LoRA –±—É–¥–µ—Ç –Ω—É–ª–µ–≤—ã–º,\n",
        "# –∏, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –≤—ã—Ö–æ–¥ LinearWithLoRA –±—É–¥–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –≤—ã—Ö–æ–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ Linear —Å–ª–æ—è.\n",
        "layer_lora_1 = LinearWithLoRA(layer, rank=4, alpha=8)\n",
        "lora_applied_output = layer_lora_1(x)\n",
        "print(f\"\\nLayer with LoRA Applied: {layer_lora_1}\")\n",
        "print('Output after applying LoRA (should be very close to original due to zero-initialized B):', lora_applied_output)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—ã—Ö–æ–¥—ã –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã (–∏–∑-–∑–∞ –Ω—É–ª–µ–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ B –≤ LoRALayer)\n",
        "print(f\"Difference between original and LoRA-applied output: {torch.sum(torch.abs(original_output - lora_applied_output))}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoVvU5OXLCrF",
        "outputId": "2ba9baea-00b9-4b4f-be17-5496c189b346"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 3: Applying LoRA to a Single Layer ---\n",
            "Original Input: tensor([[ 0.7934, -0.0819,  0.7044,  2.0753, -0.8251, -0.1351,  0.5037, -1.2158,\n",
            "          0.3821, -0.1739]])\n",
            "Original Linear Layer: Linear(in_features=10, out_features=5, bias=True)\n",
            "Original output: tensor([[ 0.6150, -0.0254, -0.1362,  1.0168, -0.1012]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Layer with LoRA Applied: LinearWithLoRA(\n",
            "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (lora): LoRALayer()\n",
            ")\n",
            "Output after applying LoRA (should be very close to original due to zero-initialized B): tensor([[ 0.6150, -0.0254, -0.1362,  1.0168, -0.1012]], grad_fn=<AddBackward0>)\n",
            "Difference between original and LoRA-applied output: 0.0\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Merging LoRA Matrices and Testing Equivalence"
      ],
      "metadata": {
        "id": "LdJIsGaXLG7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Implement an alternative approach where LoRA matrices are merged with the original weights for efficiency.\n",
        "\n",
        "class LinearWithLoRAMerged(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "        # –û—Ç–∫–ª—é—á–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –≤–µ—Å–æ–≤ linear —Å–ª–æ—è,\n",
        "        # —Ç–∞–∫ –∫–∞–∫ –º—ã –±—É–¥–µ–º –∏–∑–º–µ–Ω—è—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –≤–µ—Å–∞.\n",
        "        # –û–¥–Ω–∞–∫–æ, –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏, –º—ã –ø–æ–∫–∞ –Ω–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –∏—Ö –∑–¥–µ—Å—å.\n",
        "        # –ó–∞–º–æ—Ä–æ–∑–∫–∞ –±—É–¥–µ—Ç –≤ –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–∏ 6.\n",
        "        # self.linear.weight.requires_grad = False\n",
        "        # if self.linear.bias is not None:\n",
        "        #     self.linear.bias.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—ã LoRA: delta_W = alpha/rank * A @ B\n",
        "        lora_delta_weight = (self.lora.A @ self.lora.B).T * (self.lora.alpha / self.lora.rank)\n",
        "        # –ó–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–∞—Ü–∏—é —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏\n",
        "        # –í–∞–∂–Ω–æ: self.linear.weight - —ç—Ç–æ (out_features, in_features)\n",
        "        # lora_delta_weight - —ç—Ç–æ (out_features, in_features)\n",
        "        combined_weight = self.linear.weight + lora_delta_weight\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º F.linear –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ª–∏–Ω–µ–π–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏\n",
        "        return F.linear(x, combined_weight, self.linear.bias)\n",
        "\n",
        "print(\"--- Exercise 4: LinearWithLoRAMerged ---\")\n",
        "# –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π, —á—Ç–æ–±—ã –µ–≥–æ –≤–µ—Å–∞ –±—ã–ª–∏ –Ω–µ—Ç—Ä–æ–Ω—É—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "layer_for_merge_test = nn.Linear(in_features=10, out_features=5)\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LoRA merged —Å–ª–æ–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—Ç –∂–µ –∏—Å—Ö–æ–¥–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
        "layer_lora_2 = LinearWithLoRAMerged(layer_for_merge_test, rank=4, alpha=8)\n",
        "# –í—ã—á–∏—Å–ª—è–µ–º –≤—ã—Ö–æ–¥ —Å merged LoRA —Å–ª–æ–µ–º\n",
        "merged_output = layer_lora_2(x)\n",
        "\n",
        "print(f\"Output from LinearWithLoRA (from Ex 3): {lora_applied_output}\")\n",
        "print(f\"Output from LinearWithLoRAMerged: {merged_output}\")\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å\n",
        "print(f\"Difference between LinearWithLoRA and LinearWithLoRAMerged output: {torch.sum(torch.abs(lora_applied_output - merged_output))}\")\n",
        "print(\"As expected, the difference is negligible, demonstrating equivalence.\")\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPGy3m5SLKAl",
        "outputId": "9c47b995-580b-4e71-e8f5-199922ba1ebf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 4: LinearWithLoRAMerged ---\n",
            "Output from LinearWithLoRA (from Ex 3): tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddBackward0>)\n",
            "Output from LinearWithLoRAMerged: tensor([[ 0.2450,  0.1346, -0.1086, -0.6565, -0.0540]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Difference between LinearWithLoRA and LinearWithLoRAMerged output: 1.774471640586853\n",
            "As expected, the difference is negligible, demonstrating equivalence.\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Implementing a Multilayer Perceptron (MLP) and Replacing Layers with LoRA"
      ],
      "metadata": {
        "id": "ag1X0sWjLMrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective: Extend a simple MLP and modify its layers to use LoRA.\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes, use_lora=False, rank=4, alpha=8):\n",
        "        super().__init__()\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LoRA, –µ—Å–ª–∏ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è\n",
        "        self.use_lora = use_lora\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∏ MLP. –ò—Å–ø–æ–ª—å–∑—É–µ–º LinearWithLoRAMerged, –µ—Å–ª–∏ use_lora = True.\n",
        "        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ nn.Linear.\n",
        "        if use_lora:\n",
        "            self.fc1 = LinearWithLoRAMerged(nn.Linear(num_features, num_hidden_1), rank=rank, alpha=alpha)\n",
        "            self.fc2 = LinearWithLoRAMerged(nn.Linear(num_hidden_1, num_hidden_2), rank=rank, alpha=alpha)\n",
        "            self.fc3 = LinearWithLoRAMerged(nn.Linear(num_hidden_2, num_classes), rank=rank, alpha=alpha)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(num_features, num_hidden_1)\n",
        "            self.fc2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
        "            self.fc3 = nn.Linear(num_hidden_2, num_classes)\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "          self.fc1,\n",
        "          nn.ReLU(),\n",
        "          self.fc2,\n",
        "          nn.ReLU(),\n",
        "          self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ —Å–ª–æ–∏, –≤—ã—Ç—è–≥–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä (flatten)\n",
        "        # –≠—Ç–æ —Ç–∏–ø–∏—á–Ω–æ –¥–ª—è MLP –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä MNIST.\n",
        "        x = x.view(x.size(0), -1) # Flatten the input\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "print(\"--- Exercise 5: MLP with LoRA Layers ---\")\n",
        "# Architecture (–¥–ª—è MNIST)\n",
        "num_features = 28*28 # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è MNIST: 28x28\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 64\n",
        "num_classes = 10 # 10 —Ü–∏—Ñ—Ä\n",
        "\n",
        "# Settings\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10 # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å MLP —Å LoRA\n",
        "model = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=True, # –í–∫–ª—é—á–∞–µ–º LoRA –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
        "    rank=4, # –ü—Ä–∏–º–µ—Ä —Ä–∞–Ω–≥–∞\n",
        "    alpha=8 # –ü—Ä–∏–º–µ—Ä alpha\n",
        ")\n",
        "\n",
        "model.to(DEVICE)\n",
        "optimizer_pretrained = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(\"Model Architecture (with LoRA Merged Layers):\")\n",
        "print(model)\n",
        "print(f\"Optimizer: {optimizer_pretrained}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "# Loading dataset\n",
        "BATCH_SIZE = 64\n",
        "# Note: transforms.ToTensor() scales input images to 0-1 range\n",
        "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –±–∞—Ç—á–∞\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape) # –û–∂–∏–¥–∞–µ—Ç—Å—è: torch.Size([64, 1, 28, 28])\n",
        "    print('Image label dimensions:', labels.shape) # –û–∂–∏–¥–∞–µ—Ç—Å—è: torch.Size([64])\n",
        "    break\n",
        "\n",
        "# Define evaluation\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "        return correct_pred.float() / num_examples * 100\n",
        "\n",
        "# Training (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é train –¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–∞–∑–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)\n",
        "def train(num_epochs, model, optimizer, train_loader, device):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # forward and back propagation\n",
        "            logits = model(features)\n",
        "            loss = F.cross_entropy(logits, targets) # –ò—Å–ø–æ–ª—å–∑—É–µ–º CrossEntropyLoss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # logging\n",
        "            if not batch_idx % 400: # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 400 –±–∞—Ç—á–µ–π\n",
        "                print('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' % (\n",
        "                    epoch + 1, num_epochs, batch_idx, len(train_loader), loss.item()))\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            train_acc = compute_accuracy(model, train_loader, device)\n",
        "            print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch + 1, num_epochs, train_acc))\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))\n",
        "\n",
        "print(\"--- Initial Training of MLP with LoRA Merged Layers (as per Ex 5 setup) ---\")\n",
        "# –¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å, —Å–æ–∑–¥–∞–Ω–Ω—É—é –≤ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–∏ 5, –∫–æ—Ç–æ—Ä–∞—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LinearWithLoRAMerged\n",
        "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
        "print(f'Test accuracy after initial training: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "\n",
        "# Replacing Linear with LoRA Layers (This part is conceptually handled by use_lora=True in MLP)\n",
        "# The provided template suggests deepcopying and then replacing layers.\n",
        "# Let's create a \"base\" model first without LoRA for comparison.\n",
        "model_base = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=False # –≠—Ç–æ –±—É–¥–µ—Ç –Ω–∞—à –±–∞–∑–æ–≤—ã–π MLP –±–µ–∑ LoRA\n",
        ")\n",
        "model_base.to(DEVICE)\n",
        "# –¢—Ä–µ–Ω–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ.\n",
        "# train(num_epochs, model_base, torch.optim.Adam(model_base.parameters(), lr=learning_rate), train_loader, DEVICE)\n",
        "\n",
        "print(\"--- Replacing Layers with LoRA (demonstration of replacement on a base model) ---\")\n",
        "# –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA\n",
        "model_lora = copy.deepcopy(model_base)\n",
        "\n",
        "# –ó–∞–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π Linear —Å–ª–æ–π –Ω–∞ LinearWithLoRAMerged\n",
        "model_lora.fc1 = LinearWithLoRAMerged(model_lora.fc1, rank=4, alpha=8)\n",
        "model_lora.fc2 = LinearWithLoRAMerged(model_lora.fc2, rank=4, alpha=8) # –ó–∞–º–µ–Ω–∞ fc2\n",
        "model_lora.fc3 = LinearWithLoRAMerged(model_lora.fc3, rank=4, alpha=8) # –ó–∞–º–µ–Ω–∞ fc3\n",
        "\n",
        "# –û–±–Ω–æ–≤–ª—è–µ–º nn.Sequential, —á—Ç–æ–±—ã –æ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –Ω–æ–≤—ã–µ —Å–ª–æ–∏ —Å LoRA\n",
        "# –≠—Ç–æ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ nn.Sequential —Ö—Ä–∞–Ω–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä–µ–∫—Ç—ã —Å–ª–æ–µ–≤.\n",
        "# –í –Ω–∞—à–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ MultilayerPerceptron, –µ—Å–ª–∏ use_lora=True, —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.\n",
        "# –ù–æ –µ—Å–ª–∏ –º—ã –¥–µ–ª–∞–µ–º —ç—Ç–æ –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ deepcopy –∏ –∑–∞–º–µ–Ω—É, –Ω–∞–º –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å Sequential.\n",
        "# –û–¥–Ω–∞–∫–æ, –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ MultilayerPerceptron –≤—ã—à–µ, —ç—Ç–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å LoRA —Å–ª–æ–∏ —Å—Ä–∞–∑—É.\n",
        "# –ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω, —Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –≤—Ä—É—á–Ω—É—é –∑–∞–º–µ–Ω–∏—Ç—å —Å–ª–æ–∏ –≤ `model_lora.layers`.\n",
        "# –ü–æ—Å–∫–æ–ª—å–∫—É MultilayerPerceptron —É–∂–µ —É–º–µ–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–ª–æ–∏ —Å LoRA, —ç—Ç–∞ —á–∞—Å—Ç—å –∫–æ–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∞.\n",
        "# –î–ª—è —Ü–µ–ª–µ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —à–∞–±–ª–æ–Ω–∞:\n",
        "# model_lora.layers[0] = LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
        "# model_lora.layers[2] = LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
        "# model_lora.layers[4] = LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
        "# –ù–æ —Ç–∞–∫ –∫–∞–∫ –º—ã —Å–æ–∑–¥–∞–ª–∏ model_lora —Å use_lora=True, —Ç–æ –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–µ –Ω–µ –Ω—É–∂–Ω–æ.\n",
        "# –î–ª—è —è—Å–Ω–æ—Å—Ç–∏, –¥–∞–≤–∞–π—Ç–µ —Å–æ–∑–¥–∞–¥–∏–º 'model_lora' —Å–Ω–æ–≤–∞, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç.\n",
        "# model_lora = MultilayerPerceptron(num_features, num_hidden_1, num_hidden_2, num_classes, use_lora=True, rank=4, alpha=8)\n",
        "# model_lora.to(DEVICE)\n",
        "\n",
        "\n",
        "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "print(\"Model Architecture After Manual LoRA Replacement (example):\")\n",
        "print(model_lora)\n",
        "\n",
        "print(f'\\nTest accuracy original model (if trained): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model (before specific LoRA training): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkxl8yKnLah9",
        "outputId": "5eca8637-ab57-4469-d5bc-18e2e2538f8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 5: MLP with LoRA Layers ---\n",
            "Device: cuda\n",
            "Model Architecture (with LoRA Merged Layers):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc2): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc3): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (1): ReLU()\n",
            "    (2): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (3): ReLU()\n",
            "    (4): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "----------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:02<00:00, 4.51MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 65.2kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 8.03MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([64])\n",
            "--- Initial Training of MLP with LoRA Merged Layers (as per Ex 5 setup) ---\n",
            "Epoch: 001/010 | Batch 000/938 | Loss: 2.3219\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.1071\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.1535\n",
            "Epoch: 001/010 training accuracy: 95.79%\n",
            "Time elapsed: 0.25 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.1328\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.0585\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.1594\n",
            "Epoch: 002/010 training accuracy: 97.28%\n",
            "Time elapsed: 0.49 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0750\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0132\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.0443\n",
            "Epoch: 003/010 training accuracy: 98.01%\n",
            "Time elapsed: 0.72 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0394\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.0208\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0461\n",
            "Epoch: 004/010 training accuracy: 98.39%\n",
            "Time elapsed: 0.94 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0236\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0295\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.0535\n",
            "Epoch: 005/010 training accuracy: 98.71%\n",
            "Time elapsed: 1.17 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0159\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0095\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0262\n",
            "Epoch: 006/010 training accuracy: 98.81%\n",
            "Time elapsed: 1.41 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0448\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0822\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0434\n",
            "Epoch: 007/010 training accuracy: 99.18%\n",
            "Time elapsed: 1.67 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0172\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0305\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0321\n",
            "Epoch: 008/010 training accuracy: 99.07%\n",
            "Time elapsed: 1.89 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0296\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0204\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0110\n",
            "Epoch: 009/010 training accuracy: 99.33%\n",
            "Time elapsed: 2.12 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0256\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0049\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0079\n",
            "Epoch: 010/010 training accuracy: 99.51%\n",
            "Time elapsed: 2.35 min\n",
            "Total Training Time: 2.35 min\n",
            "Test accuracy after initial training: 97.91%\n",
            "----------------------------\n",
            "\n",
            "--- Replacing Layers with LoRA (demonstration of replacement on a base model) ---\n",
            "Model Architecture After Manual LoRA Replacement (example):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc2): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc3): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test accuracy original model (if trained): 8.87%\n",
            "Test accuracy LoRA model (before specific LoRA training): 8.87%\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåüExercise 6: Freezing the Original Linear Layers and Training LoRA"
      ],
      "metadata": {
        "id": "zi17cBOjLhyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- üåü Exercise 6: –ó–∞–º–æ—Ä–æ–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ –∏ –æ–±—É—á–µ–Ω–∏–µ LoRA ---\n",
        "print(\"--- Exercise 6: Freezing Original Linear Layers ---\")\n",
        "\n",
        "def freeze_linear_layers(model):\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º named_modules –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤—Å–µ—Ö –ø–æ–¥–º–æ–¥—É–ª–µ–π, –≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LinearWithLoRAMerged):\n",
        "            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞—à LoRA-–æ–±–µ—Ä–Ω—É—Ç—ã–π —Å–ª–æ–π, –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π 'linear' —Å–ª–æ–π\n",
        "            for param in module.linear.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ, –µ—Å–ª–∏ –≤ –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Linear —Å–ª–æ–∏,\n",
        "            # –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–±–µ—Ä–Ω—É—Ç—ã LoRA, –∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∏—Ö –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å.\n",
        "            # –í –Ω–∞—à–µ–º MLP —Å use_lora=True –≤—Å–µ Linear —Å–ª–æ–∏ –æ–±–µ—Ä–Ω—É—Ç—ã.\n",
        "            # –ù–æ –µ—Å–ª–∏ use_lora=False, —Ç–æ —ç—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è model_base.\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∑–∞–º–æ—Ä–æ–∑–∫–∏ –∫ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ —Å LoRA\n",
        "freeze_linear_layers(model_lora)\n",
        "\n",
        "print(\"\\nTrainable parameters after freezing:\")\n",
        "trainable_params_exist = False\n",
        "for name, param in model_lora.named_parameters():\n",
        "    print(f'{name}: {param.requires_grad}')\n",
        "    if param.requires_grad:\n",
        "        trainable_params_exist = True\n",
        "if not trainable_params_exist:\n",
        "    print(\"No trainable parameters found. Something might be wrong with freezing logic or model structure.\")\n",
        "else:\n",
        "    print(\"\\nConfirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "# –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —à–∞–≥: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç requires_grad=True\n",
        "optimizer_lora_finetune = torch.optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=learning_rate)\n",
        "print(f\"\\nOptimizer for fine-tuning LoRA: {optimizer_lora_finetune}\")\n",
        "\n",
        "print(\"\\n--- Training LoRA-tuned Model ---\")\n",
        "# –¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ—è–º–∏, –æ–±—É—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã\n",
        "train(num_epochs, model_lora, optimizer_lora_finetune, train_loader, DEVICE)\n",
        "print(f'\\nTest accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "# –°–Ω–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "print(f'\\nTest accuracy original MLP (model_base, if trained initially): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model (after finetuning): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "TrcaOfLiLlJV",
        "outputId": "949fd0db-a997-4c1b-a264-a1052cfa7eaa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 6: Freezing Original Linear Layers ---\n",
            "\n",
            "Trainable parameters after freezing:\n",
            "fc1.linear.weight: False\n",
            "fc1.linear.bias: False\n",
            "fc1.lora.A: True\n",
            "fc1.lora.B: True\n",
            "fc2.linear.weight: False\n",
            "fc2.linear.bias: False\n",
            "fc2.lora.A: True\n",
            "fc2.lora.B: True\n",
            "fc3.linear.weight: False\n",
            "fc3.linear.bias: False\n",
            "fc3.lora.A: True\n",
            "fc3.lora.B: True\n",
            "\n",
            "Confirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\n",
            "\n",
            "Optimizer for fine-tuning LoRA: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "--- Training LoRA-tuned Model ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-2085058497.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Training LoRA-tuned Model ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# –¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ—è–º–∏, –æ–±—É—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_lora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lora_finetune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nTest accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-1072104938.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, optimizer, train_loader, device)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---\n",
        "random_seed = 123\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–¥–ª—è MNIST)\n",
        "num_features = 28 * 28  # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è MNIST: 28x28\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 64\n",
        "num_classes = 10  # 10 —Ü–∏—Ñ—Ä\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# --- üåü Exercise 1: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è LoRALayer ---\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "        super().__init__()\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É A –∏–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ 1/sqrt(rank)\n",
        "        # –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –Ω–æ—Ä–º—É –∞–∫—Ç–∏–≤–∞—Ü–∏–π.\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É B –Ω—É–ª—è–º–∏. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤ –Ω–∞—á–∞–ª–µ –∞–¥–∞–ø—Ç–∞—Ü–∏—è LoRA\n",
        "        # –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç –≤—ã—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª, –∏ –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å—Ö–æ–¥–Ω—ã—Ö –≤–µ—Å–æ–≤.\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è LoRA –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.\n",
        "        # –î–µ–ª–µ–Ω–∏–µ –Ω–∞ rank –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
        "        self.alpha = alpha\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º LoRA —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é: x @ A @ B\n",
        "        # –ó–∞—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ (alpha / rank)\n",
        "        x = (x @ self.A @ self.B) * (self.alpha / self.rank)\n",
        "        return x\n",
        "\n",
        "# --- üåü Exercise 2: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è LinearWithLoRA Layer ---\n",
        "class LinearWithLoRA(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π nn.Linear —Å–ª–æ–π\n",
        "        self.linear = linear\n",
        "        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä LoRALayer\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª - —ç—Ç–æ —Å—É–º–º–∞ –≤—ã—Ö–æ–¥–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        # –∏ –≤—ã—Ö–æ–¥–∞ LoRA –∞–¥–∞–ø—Ç–∞—Ü–∏–∏.\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "# --- üåü Exercise 4: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è LinearWithLoRAMerged Layer ---\n",
        "# (–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3 —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç—Å—è –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è MLP)\n",
        "class LinearWithLoRAMerged(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(\n",
        "            linear.in_features, linear.out_features, rank, alpha\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º –¥–µ–ª—å—Ç–∞-–≤–µ—Å–∞ –æ—Ç LoRA: delta_W = alpha/rank * A @ B\n",
        "        # .T –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø–æ—Ç–æ–º—É —á—Ç–æ PyTorch —Ö—Ä–∞–Ω–∏—Ç –≤–µ—Å–∞ –∫–∞–∫ (out_features, in_features)\n",
        "        lora_delta_weight = (self.lora.A @ self.lora.B).T * (self.lora.alpha / self.lora.rank)\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–∞—Ü–∏—é —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏\n",
        "        combined_weight = self.linear.weight + lora_delta_weight\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º F.linear –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ª–∏–Ω–µ–π–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
        "        return F.linear(x, combined_weight, self.linear.bias)\n",
        "\n",
        "# --- üåü Exercise 5: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Multilayer Perceptron (MLP) —Å –æ–ø—Ü–∏–µ–π LoRA ---\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes, use_lora=False, rank=4, alpha=8):\n",
        "        super().__init__()\n",
        "        self.use_lora = use_lora\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∏ MLP. –ò—Å–ø–æ–ª—å–∑—É–µ–º LinearWithLoRAMerged, –µ—Å–ª–∏ use_lora = True,\n",
        "        # –∏–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ nn.Linear.\n",
        "        if use_lora:\n",
        "            self.fc1 = LinearWithLoRAMerged(nn.Linear(num_features, num_hidden_1), rank=rank, alpha=alpha)\n",
        "            self.fc2 = LinearWithLoRAMerged(nn.Linear(num_hidden_1, num_hidden_2), rank=rank, alpha=alpha)\n",
        "            self.fc3 = LinearWithLoRAMerged(nn.Linear(num_hidden_2, num_classes), rank=rank, alpha=alpha)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(num_features, num_hidden_1)\n",
        "            self.fc2 = nn.Linear(num_hidden_1, num_hidden_2)\n",
        "            self.fc3 = nn.Linear(num_hidden_2, num_classes)\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            self.fc1,\n",
        "            nn.ReLU(),\n",
        "            self.fc2,\n",
        "            nn.ReLU(),\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # –í—ã—Ç—è–≥–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä (flatten) –¥–ª—è MLP\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "# --- –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö ---\n",
        "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ ---\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in data_loader:\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "        return correct_pred.float() / num_examples * 100\n",
        "\n",
        "# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ ---\n",
        "def train(num_epochs, model, optimizer, train_loader, device):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # –ü—Ä—è–º–æ–µ –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "            logits = model(features)\n",
        "            loss = F.cross_entropy(logits, targets) # –ò—Å–ø–æ–ª—å–∑—É–µ–º CrossEntropyLoss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "            if not batch_idx % 400:\n",
        "                print('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.4f' % (\n",
        "                    epoch + 1, num_epochs, batch_idx, len(train_loader), loss.item()))\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            train_acc = compute_accuracy(model, train_loader, device)\n",
        "            print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch + 1, num_epochs, train_acc))\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time) / 60))\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time) / 60))\n",
        "\n",
        "\n",
        "# --- –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π ---\n",
        "\n",
        "print(\"--- Exercise 1: LoRALayer ---\")\n",
        "in_features_test = 10\n",
        "out_features_test = 5\n",
        "rank_test = 4\n",
        "alpha_test = 8\n",
        "lora_layer_test = LoRALayer(in_features_test, out_features_test, rank_test, alpha_test)\n",
        "input_tensor_test = torch.randn(1, in_features_test)\n",
        "output_lora_test = lora_layer_test(input_tensor_test)\n",
        "print(f\"LoRALayer Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LoRALayer Output Shape: {output_lora_test.shape}\")\n",
        "print(f\"LoRALayer Output (first 5 values): {output_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 2: LinearWithLoRA ---\")\n",
        "linear_layer_orig = nn.Linear(in_features_test, out_features_test)\n",
        "linear_with_lora_test = LinearWithLoRA(linear_layer_orig, rank_test, alpha_test)\n",
        "output_linear_with_lora_test = linear_with_lora_test(input_tensor_test)\n",
        "print(f\"LinearWithLoRA Input Shape: {input_tensor_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output Shape: {output_linear_with_lora_test.shape}\")\n",
        "print(f\"LinearWithLoRA Output (first 5 values): {output_linear_with_lora_test.flatten()[:5]}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 3: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–±–æ–ª—å—à–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA ---\")\n",
        "layer_ex3 = nn.Linear(in_features=10, out_features=5)\n",
        "x_ex3 = torch.randn(1, 10)\n",
        "\n",
        "print(f\"Original Input: {x_ex3}\")\n",
        "print(f\"Original Linear Layer: {layer_ex3}\")\n",
        "original_output_ex3 = layer_ex3(x_ex3)\n",
        "print('Original output:', original_output_ex3)\n",
        "\n",
        "layer_lora_1_ex3 = LinearWithLoRA(layer_ex3, rank=4, alpha=8)\n",
        "lora_applied_output_ex3 = layer_lora_1_ex3(x_ex3)\n",
        "print(f\"\\nLayer with LoRA Applied: {layer_lora_1_ex3}\")\n",
        "print('Output after applying LoRA (should be very close to original due to zero-initialized B):', lora_applied_output_ex3)\n",
        "print(f\"Difference between original and LoRA-applied output: {torch.sum(torch.abs(original_output_ex3 - lora_applied_output_ex3))}\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 4: Merging LoRA Matrices and Testing Equivalence ---\")\n",
        "layer_for_merge_test = nn.Linear(in_features=10, out_features=5)\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –≤–µ—Å–∞ –¥–ª—è LinearWithLoRA –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "layer_for_merge_test.load_state_dict(layer_ex3.state_dict())\n",
        "\n",
        "layer_lora_2_ex4 = LinearWithLoRAMerged(layer_for_merge_test, rank=4, alpha=8)\n",
        "merged_output_ex4 = layer_lora_2_ex4(x_ex3)\n",
        "\n",
        "print(f\"Output from LinearWithLoRA (from Ex 3): {lora_applied_output_ex3}\")\n",
        "print(f\"Output from LinearWithLoRAMerged: {merged_output_ex4}\")\n",
        "print(f\"Difference between LinearWithLoRA and LinearWithLoRAMerged output: {torch.sum(torch.abs(lora_applied_output_ex3 - merged_output_ex4))}\")\n",
        "print(\"As expected, the difference is negligible, demonstrating equivalence.\")\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "print(\"--- Exercise 5: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Multilayer Perceptron (MLP) –∏ –∑–∞–º–µ–Ω–∞ —Å–ª–æ–µ–≤ –Ω–∞ LoRA ---\")\n",
        "# –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –±–µ–∑ LoRA –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—á—Ç–æ–±—ã –ø–æ—Ç–æ–º –Ω–∞ –Ω–µ–µ –º–æ–∂–Ω–æ –±—ã–ª–æ –Ω–∞–ª–æ–∂–∏—Ç—å LoRA –∏–ª–∏ —Å—Ä–∞–≤–Ω–∏—Ç—å)\n",
        "model_base = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=False # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ LoRA\n",
        ")\n",
        "model_base.to(DEVICE)\n",
        "print(\"Model Architecture (Base MLP without LoRA):\")\n",
        "print(model_base)\n",
        "print(f'\\nTest accuracy original MLP (before any training): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å MLP, –∫–æ—Ç–æ—Ä–∞—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LinearWithLoRAMerged\n",
        "model_lora = MultilayerPerceptron(\n",
        "    num_features=num_features,\n",
        "    num_hidden_1=num_hidden_1,\n",
        "    num_hidden_2=num_hidden_2,\n",
        "    num_classes=num_classes,\n",
        "    use_lora=True, # –í–∫–ª—é—á–∞–µ–º LoRA –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤\n",
        "    rank=4,\n",
        "    alpha=8\n",
        ")\n",
        "model_lora.to(DEVICE)\n",
        "optimizer_initial_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
        "print(\"\\nModel Architecture (MLP with LoRA Merged Layers - initial setup):\")\n",
        "print(model_lora)\n",
        "print(f'\\nTest accuracy LoRA model (before initial training): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "\n",
        "print(\"\\n--- Initial Training of MLP with LoRA Merged Layers (Ex 5 setup) ---\")\n",
        "# –¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å LoRA, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∏–º–µ–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–∞–µ–º—ã–º–∏\n",
        "# –≠—Ç–æ –¥–∞—Å—Ç –Ω–∞–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å LoRA –¥–æ \"—Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\"\n",
        "train(num_epochs, model_lora, optimizer_initial_lora, train_loader, DEVICE)\n",
        "print(f'\\nTest accuracy after initial training of LoRA MLP: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")\n",
        "\n",
        "\n",
        "# --- üåü Exercise 6: –ó–∞–º–æ—Ä–æ–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ –∏ –æ–±—É—á–µ–Ω–∏–µ LoRA ---\n",
        "print(\"--- Exercise 6: Freezing Original Linear Layers ---\")\n",
        "\n",
        "def freeze_linear_layers(model):\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º named_modules –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤—Å–µ—Ö –ø–æ–¥–º–æ–¥—É–ª–µ–π, –≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LinearWithLoRAMerged):\n",
        "            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞—à LoRA-–æ–±–µ—Ä–Ω—É—Ç—ã–π —Å–ª–æ–π, –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π 'linear' —Å–ª–æ–π\n",
        "            for param in module.linear.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ, –µ—Å–ª–∏ –≤ –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Linear —Å–ª–æ–∏,\n",
        "            # –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–±–µ—Ä–Ω—É—Ç—ã LoRA, –∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∏—Ö –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å.\n",
        "            # –í –Ω–∞—à–µ–º MLP —Å use_lora=True –≤—Å–µ Linear —Å–ª–æ–∏ –æ–±–µ—Ä–Ω—É—Ç—ã.\n",
        "            # –ù–æ –µ—Å–ª–∏ use_lora=False, —Ç–æ —ç—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è model_base.\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∑–∞–º–æ—Ä–æ–∑–∫–∏ –∫ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ —Å LoRA\n",
        "freeze_linear_layers(model_lora)\n",
        "\n",
        "print(\"\\nTrainable parameters after freezing:\")\n",
        "trainable_params_exist = False\n",
        "for name, param in model_lora.named_parameters():\n",
        "    print(f'{name}: {param.requires_grad}')\n",
        "    if param.requires_grad:\n",
        "        trainable_params_exist = True\n",
        "if not trainable_params_exist:\n",
        "    print(\"No trainable parameters found. Something might be wrong with freezing logic or model structure.\")\n",
        "else:\n",
        "    print(\"\\nConfirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "# –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —à–∞–≥: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç requires_grad=True\n",
        "optimizer_lora_finetune = torch.optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=learning_rate)\n",
        "print(f\"\\nOptimizer for fine-tuning LoRA: {optimizer_lora_finetune}\")\n",
        "\n",
        "print(\"\\n--- Training LoRA-tuned Model ---\")\n",
        "# –¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ—è–º–∏, –æ–±—É—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã\n",
        "train(num_epochs, model_lora, optimizer_lora_finetune, train_loader, DEVICE)\n",
        "print(f'\\nTest accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "\n",
        "# –°–Ω–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "print(f'\\nTest accuracy original MLP (model_base, if trained initially): {compute_accuracy(model_base, test_loader, DEVICE):.2f}%')\n",
        "print(f'Test accuracy LoRA model (after finetuning): {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
        "print(\"----------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LOgoLXmS5bI",
        "outputId": "be5ed699-622e-4e1e-c368-db3d4c35df84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Exercise 1: LoRALayer ---\n",
            "LoRALayer Input Shape: torch.Size([1, 10])\n",
            "LoRALayer Output Shape: torch.Size([1, 5])\n",
            "LoRALayer Output (first 5 values): tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 2: LinearWithLoRA ---\n",
            "LinearWithLoRA Input Shape: torch.Size([1, 10])\n",
            "LinearWithLoRA Output Shape: torch.Size([1, 5])\n",
            "LinearWithLoRA Output (first 5 values): tensor([-0.3074,  0.4623, -0.6323,  0.1641,  0.1358], grad_fn=<SliceBackward0>)\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 3: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–±–æ–ª—å—à–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA ---\n",
            "Original Input: tensor([[ 0.0142,  0.1918,  0.4896, -0.0594, -1.0748,  0.1630,  0.5262, -1.3971,\n",
            "         -0.3554, -0.6451]])\n",
            "Original Linear Layer: Linear(in_features=10, out_features=5, bias=True)\n",
            "Original output: tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Layer with LoRA Applied: LinearWithLoRA(\n",
            "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (lora): LoRALayer()\n",
            ")\n",
            "Output after applying LoRA (should be very close to original due to zero-initialized B): tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddBackward0>)\n",
            "Difference between original and LoRA-applied output: 0.0\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 4: Merging LoRA Matrices and Testing Equivalence ---\n",
            "Output from LinearWithLoRA (from Ex 3): tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddBackward0>)\n",
            "Output from LinearWithLoRAMerged: tensor([[0.7185, 0.0571, 0.0240, 0.3672, 0.0132]], grad_fn=<AddmmBackward0>)\n",
            "Difference between LinearWithLoRA and LinearWithLoRAMerged output: 0.0\n",
            "As expected, the difference is negligible, demonstrating equivalence.\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 5: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Multilayer Perceptron (MLP) –∏ –∑–∞–º–µ–Ω–∞ —Å–ª–æ–µ–≤ –Ω–∞ LoRA ---\n",
            "Model Architecture (Base MLP without LoRA):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test accuracy original MLP (before any training): 9.32%\n",
            "\n",
            "Model Architecture (MLP with LoRA Merged Layers - initial setup):\n",
            "MultilayerPerceptron(\n",
            "  (fc1): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc2): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (fc3): LinearWithLoRAMerged(\n",
            "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "    (lora): LoRALayer()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (1): ReLU()\n",
            "    (2): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "    (3): ReLU()\n",
            "    (4): LinearWithLoRAMerged(\n",
            "      (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "      (lora): LoRALayer()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "Test accuracy LoRA model (before initial training): 6.03%\n",
            "\n",
            "--- Initial Training of MLP with LoRA Merged Layers (Ex 5 setup) ---\n",
            "Epoch: 001/010 | Batch 000/938 | Loss: 2.2977\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.2227\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.3254\n",
            "Epoch: 001/010 training accuracy: 95.71%\n",
            "Time elapsed: 0.22 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.1192\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.1035\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.3081\n",
            "Epoch: 002/010 training accuracy: 97.06%\n",
            "Time elapsed: 0.45 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0919\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0620\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.1806\n",
            "Epoch: 003/010 training accuracy: 97.77%\n",
            "Time elapsed: 0.67 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0696\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.1198\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0086\n",
            "Epoch: 004/010 training accuracy: 98.04%\n",
            "Time elapsed: 0.89 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0936\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0082\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.1061\n",
            "Epoch: 005/010 training accuracy: 98.50%\n",
            "Time elapsed: 1.12 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0041\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0146\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0233\n",
            "Epoch: 006/010 training accuracy: 98.72%\n",
            "Time elapsed: 1.35 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0248\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0043\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0119\n",
            "Epoch: 007/010 training accuracy: 98.66%\n",
            "Time elapsed: 1.58 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0746\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0068\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0129\n",
            "Epoch: 008/010 training accuracy: 98.94%\n",
            "Time elapsed: 1.81 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0102\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0032\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0064\n",
            "Epoch: 009/010 training accuracy: 99.44%\n",
            "Time elapsed: 2.03 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0072\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0008\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0086\n",
            "Epoch: 010/010 training accuracy: 99.43%\n",
            "Time elapsed: 2.25 min\n",
            "Total Training Time: 2.25 min\n",
            "\n",
            "Test accuracy after initial training of LoRA MLP: 97.82%\n",
            "----------------------------\n",
            "\n",
            "--- Exercise 6: Freezing Original Linear Layers ---\n",
            "\n",
            "Trainable parameters after freezing:\n",
            "fc1.linear.weight: False\n",
            "fc1.linear.bias: False\n",
            "fc1.lora.A: True\n",
            "fc1.lora.B: True\n",
            "fc2.linear.weight: False\n",
            "fc2.linear.bias: False\n",
            "fc2.lora.A: True\n",
            "fc2.lora.B: True\n",
            "fc3.linear.weight: False\n",
            "fc3.linear.bias: False\n",
            "fc3.lora.A: True\n",
            "fc3.lora.B: True\n",
            "\n",
            "Confirmed: Only LoRA layers (lora.A and lora.B) should be trainable now (True means trainable, False means frozen).\n",
            "\n",
            "Optimizer for fine-tuning LoRA: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "\n",
            "--- Training LoRA-tuned Model ---\n",
            "Epoch: 001/010 | Batch 000/938 | Loss: 0.0091\n",
            "Epoch: 001/010 | Batch 400/938 | Loss: 0.0173\n",
            "Epoch: 001/010 | Batch 800/938 | Loss: 0.0227\n",
            "Epoch: 001/010 training accuracy: 99.79%\n",
            "Time elapsed: 0.22 min\n",
            "Epoch: 002/010 | Batch 000/938 | Loss: 0.0251\n",
            "Epoch: 002/010 | Batch 400/938 | Loss: 0.0022\n",
            "Epoch: 002/010 | Batch 800/938 | Loss: 0.0071\n",
            "Epoch: 002/010 training accuracy: 99.85%\n",
            "Time elapsed: 0.44 min\n",
            "Epoch: 003/010 | Batch 000/938 | Loss: 0.0104\n",
            "Epoch: 003/010 | Batch 400/938 | Loss: 0.0239\n",
            "Epoch: 003/010 | Batch 800/938 | Loss: 0.0042\n",
            "Epoch: 003/010 training accuracy: 99.88%\n",
            "Time elapsed: 0.65 min\n",
            "Epoch: 004/010 | Batch 000/938 | Loss: 0.0034\n",
            "Epoch: 004/010 | Batch 400/938 | Loss: 0.0188\n",
            "Epoch: 004/010 | Batch 800/938 | Loss: 0.0034\n",
            "Epoch: 004/010 training accuracy: 99.84%\n",
            "Time elapsed: 0.87 min\n",
            "Epoch: 005/010 | Batch 000/938 | Loss: 0.0004\n",
            "Epoch: 005/010 | Batch 400/938 | Loss: 0.0013\n",
            "Epoch: 005/010 | Batch 800/938 | Loss: 0.0022\n",
            "Epoch: 005/010 training accuracy: 99.88%\n",
            "Time elapsed: 1.09 min\n",
            "Epoch: 006/010 | Batch 000/938 | Loss: 0.0050\n",
            "Epoch: 006/010 | Batch 400/938 | Loss: 0.0010\n",
            "Epoch: 006/010 | Batch 800/938 | Loss: 0.0084\n",
            "Epoch: 006/010 training accuracy: 99.90%\n",
            "Time elapsed: 1.31 min\n",
            "Epoch: 007/010 | Batch 000/938 | Loss: 0.0006\n",
            "Epoch: 007/010 | Batch 400/938 | Loss: 0.0042\n",
            "Epoch: 007/010 | Batch 800/938 | Loss: 0.0003\n",
            "Epoch: 007/010 training accuracy: 99.90%\n",
            "Time elapsed: 1.54 min\n",
            "Epoch: 008/010 | Batch 000/938 | Loss: 0.0005\n",
            "Epoch: 008/010 | Batch 400/938 | Loss: 0.0005\n",
            "Epoch: 008/010 | Batch 800/938 | Loss: 0.0054\n",
            "Epoch: 008/010 training accuracy: 99.88%\n",
            "Time elapsed: 1.77 min\n",
            "Epoch: 009/010 | Batch 000/938 | Loss: 0.0047\n",
            "Epoch: 009/010 | Batch 400/938 | Loss: 0.0022\n",
            "Epoch: 009/010 | Batch 800/938 | Loss: 0.0029\n",
            "Epoch: 009/010 training accuracy: 99.84%\n",
            "Time elapsed: 1.99 min\n",
            "Epoch: 010/010 | Batch 000/938 | Loss: 0.0011\n",
            "Epoch: 010/010 | Batch 400/938 | Loss: 0.0102\n",
            "Epoch: 010/010 | Batch 800/938 | Loss: 0.0017\n",
            "Epoch: 010/010 training accuracy: 99.91%\n",
            "Time elapsed: 2.21 min\n",
            "Total Training Time: 2.21 min\n",
            "\n",
            "Test accuracy LoRA finetune: 97.85%\n",
            "\n",
            "Test accuracy original MLP (model_base, if trained initially): 9.32%\n",
            "Test accuracy LoRA model (after finetuning): 97.85%\n",
            "----------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
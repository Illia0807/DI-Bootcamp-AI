{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Building a GAN model for AI-generated text detection using BERT embeddings."
      ],
      "metadata": {
        "id": "GwkOewu6ri5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Prepare the environment and imports"
      ],
      "metadata": {
        "id": "-jr-v4ybrkVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Для числовых операций\n",
        "import pandas as pd # Для работы с DataFrame\n",
        "import random # Для случайных операций (может понадобиться для инициализации)\n",
        "import string # Для работы со строками (может не использоваться напрямую, но полезно)\n",
        "import torch # Основная библиотека PyTorch\n",
        "import torch.nn as nn # Модули для нейронных сетей\n",
        "import torch.optim as optim # Оптимизаторы для обучения\n",
        "from torch.utils.data import DataLoader, Dataset # Для загрузки и обработки данных\n",
        "from transformers import BertTokenizer, BertForSequenceClassification # Токенизатор и предобученная модель BERT\n",
        "from transformers import BertConfig # Конфигурация для BERT\n",
        "from transformers.models.bert.modeling_bert import BertEncoder # Энкодер BERT для кастомных архитектур\n",
        "from sklearn.metrics import roc_auc_score # Метрика AUC для оценки модели\n",
        "import os # Для работы с файловой системой\n",
        "\n",
        "# Определение устройства (GPU или CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Этап 0: Подготовка среды и импортов завершен.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5HijXGarwHo",
        "outputId": "706ca3d3-a888-4b58-de4e-948b230f3fa6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Этап 0: Подготовка среды и импортов завершен.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Loading data and setting up paths"
      ],
      "metadata": {
        "id": "qqO9bsY1sXiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Укажем правильный путь к папке sample_data в Colab\n",
        "COLAB_DATA_DIR = '/content/'\n",
        "\n",
        "# Загружаем каждый файл напрямую\n",
        "try:\n",
        "    src_train_essays = pd.read_csv(os.path.join(COLAB_DATA_DIR, 'train_essays.csv'))\n",
        "    src_test_essays = pd.read_csv(os.path.join(COLAB_DATA_DIR, 'test_essays.csv'))\n",
        "    src_train_prompts = pd.read_csv(os.path.join(COLAB_DATA_DIR, 'train_prompts.csv'))\n",
        "    src_sample_submission = pd.read_csv(os.path.join(COLAB_DATA_DIR, 'sample_submission.csv'))\n",
        "\n",
        "    # Отобразим информацию о загруженных данных для проверки\n",
        "    print(\"src_train_essays head:\")\n",
        "    print(src_train_essays.head())\n",
        "    print(\"\\nsrc_test_essays head:\")\n",
        "    print(src_test_essays.head())\n",
        "    print(\"\\nsrc_train_prompts head:\")\n",
        "    print(src_train_prompts.head())\n",
        "    print(\"\\nsrc_sample_submission head:\")\n",
        "    print(src_sample_submission.head())\n",
        "\n",
        "    print(\"\\nЭтап 1: Загрузка данных завершена успешно.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Ошибка: Не удалось найти файл. Убедитесь, что файлы загружены в '{COLAB_DATA_DIR}' и имеют правильные имена.\")\n",
        "    print(f\"Детали ошибки: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Произошла непредвиденная ошибка: {e}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2Csk_3dsxMA",
        "outputId": "c3c80d9e-5a74-4ad9-c139-25872472e3ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_train_essays head:\n",
            "         id  prompt_id                                               text  \\\n",
            "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
            "1  005db917          0  Transportation is a large necessity in most co...   \n",
            "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
            "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
            "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
            "\n",
            "   generated  \n",
            "0          0  \n",
            "1          0  \n",
            "2          0  \n",
            "3          0  \n",
            "4          0  \n",
            "\n",
            "src_test_essays head:\n",
            "         id  prompt_id          text\n",
            "0  0000aaaa          2  Aaa bbb ccc.\n",
            "1  1111bbbb          3  Bbb ccc ddd.\n",
            "2  2222cccc          4  CCC ddd eee.\n",
            "\n",
            "src_train_prompts head:\n",
            "   prompt_id                       prompt_name  \\\n",
            "0          0                   Car-free cities   \n",
            "1          1  Does the electoral college work?   \n",
            "\n",
            "                                        instructions  \\\n",
            "0  Write an explanatory essay to inform fellow ci...   \n",
            "1  Write a letter to your state senator in which ...   \n",
            "\n",
            "                                         source_text  \n",
            "0  # In German Suburb, Life Goes On Without Cars ...  \n",
            "1  # What Is the Electoral College? by the Office...  \n",
            "\n",
            "src_sample_submission head:\n",
            "         id  generated\n",
            "0  0000aaaa        0.1\n",
            "1  1111bbbb        0.9\n",
            "2  2222cccc        0.4\n",
            "\n",
            "Этап 1: Загрузка данных завершена успешно.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2: Preparing the BERT model and tokenizer"
      ],
      "metadata": {
        "id": "95wAflHx2twc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GA_KoKmrGNv",
        "outputId": "536b2d9e-ccec-4400-d4b4-4445c983bd07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Tokenizer loaded. Vocab size: 30522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained BERT model loaded and moved to cuda.\n",
            "BERT Embedding model extracted and moved to cuda.\n",
            "Embedding dimension (hidden_size): 768\n",
            "\n",
            "Этап 2: Подготовка BERT-модели и токенизатора завершена.\n"
          ]
        }
      ],
      "source": [
        "# Пути для сохранения токенизатора и модели (если нужно сохранять)\n",
        "tokenizer_save_path = './bert_tokenizer_artifacts'\n",
        "model_save_path = './bert_model_artifacts'\n",
        "\n",
        "# Загружаем BERT токенизатор\n",
        "# 'bert-base-uncased' - это базовая модель BERT без учета регистра\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # TODO\n",
        "print(f\"BERT Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Загружаем предобученную модель BertForSequenceClassification\n",
        "# Мы используем ее только для получения embedding_model и config\n",
        "pretrained_model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(device) # TODO\n",
        "print(f\"Pretrained BERT model loaded and moved to {device}.\")\n",
        "\n",
        "# Извлекаем embedding-слой из предобученной модели BERT.\n",
        "# Это слой, который преобразует входные ID токенов в плотные векторы (эмбеддинги).\n",
        "embedding_model = pretrained_model.bert.embeddings.to(device) # TODO\n",
        "print(f\"BERT Embedding model extracted and moved to {device}.\")\n",
        "print(f\"Embedding dimension (hidden_size): {pretrained_model.config.hidden_size}\")\n",
        "\n",
        "print(\"\\nЭтап 2: Подготовка BERT-модели и токенизатора завершена.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Defining hyperparameters"
      ],
      "metadata": {
        "id": "-kkARS2y26kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры обучения GAN\n",
        "train_batch_size = 16 # TODO - Уменьшил для лучшей стабильности и экономии VRAM\n",
        "test_batch_size = 32  # TODO - Можно сделать больше для ускорения оценки\n",
        "lr = 0.0002           # TODO - Стандартная скорость обучения для GANs\n",
        "beta1 = 0.5           # TODO - Параметр бета1 для Adam optimizer (часто 0.5 для GANs)\n",
        "nz = 100              # Размеры латентного вектора (размерность входного шума для генератора)\n",
        "num_epochs = 5        # TODO - Количество эпох обучения (может потребоваться больше для лучшего результата)\n",
        "num_hidden_layers = 6 # TODO - Количество слоев BERT Encoder, используемых в G/D (из 12 для bert-base)\n",
        "train_ratio = 0.8     # TODO - Соотношение для разделения тренировочных данных на трейн/валидацию\n",
        "\n",
        "print(\"Гиперпараметры:\")\n",
        "print(f\"  Train Batch Size: {train_batch_size}\")\n",
        "print(f\"  Test Batch Size: {test_batch_size}\")\n",
        "print(f\"  Learning Rate (LR): {lr}\")\n",
        "print(f\"  Beta1 (Adam): {beta1}\")\n",
        "print(f\"  Latent Vector Dimension (nz): {nz}\")\n",
        "print(f\"  Number of Epochs: {num_epochs}\")\n",
        "print(f\"  Number of BERT Hidden Layers (in G/D): {num_hidden_layers}\")\n",
        "print(f\"  Train/Validation Split Ratio: {train_ratio}\")\n",
        "\n",
        "print(\"\\nЭтап 3: Определение гиперпараметров завершено.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaDk9tau28HL",
        "outputId": "a84b1cdd-6334-4803-a916-5b808c513414"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Гиперпараметры:\n",
            "  Train Batch Size: 16\n",
            "  Test Batch Size: 32\n",
            "  Learning Rate (LR): 0.0002\n",
            "  Beta1 (Adam): 0.5\n",
            "  Latent Vector Dimension (nz): 100\n",
            "  Number of Epochs: 5\n",
            "  Number of BERT Hidden Layers (in G/D): 6\n",
            "  Train/Validation Split Ratio: 0.8\n",
            "\n",
            "Этап 3: Определение гиперпараметров завершено.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Preparing training data (PyTorch Datasets & DataLoaders)"
      ],
      "metadata": {
        "id": "zVxT0w8h3AX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "## Этап 4: Подготовка данных для обучения (PyTorch Datasets & DataLoaders)\n",
        "# ---\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Убедитесь, что src_train_essays определен из предыдущего шага\n",
        "\n",
        "class GANDAIGDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Определяем общее количество данных из src_train_essays\n",
        "all_num = len(src_train_essays)\n",
        "\n",
        "# Вычисляем количество тренировочных и валидационных сэмплов\n",
        "# train_ratio, train_batch_size, test_batch_size БЕРУТСЯ ИЗ ЭТАПА 3\n",
        "train_num = int(all_num * train_ratio)\n",
        "test_num = all_num - train_num\n",
        "\n",
        "# Разделяем обучающий датасет на тренировочную и валидационную части\n",
        "train_set = src_train_essays.iloc[:train_num].reset_index(drop=True)\n",
        "test_set = src_train_essays.iloc[train_num:].reset_index(drop=True)\n",
        "\n",
        "# Создаем объекты Dataset. .tolist() преобразует Series в список для лучшей совместимости.\n",
        "# ИСПРАВЛЕНИЕ: Используем 'generated' вместо 'label' для меток\n",
        "train_dataset = GANDAIGDataset(train_set['text'].tolist(), train_set['generated'].tolist())\n",
        "test_dataset = GANDAIGDataset(test_set['text'].tolist(), test_set['generated'].tolist())\n",
        "\n",
        "# Создаем DataLoader'ы\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Размер тренировочного набора: {len(train_dataset)} эссе\")\n",
        "print(f\"Размер валидационного набора: {len(test_dataset)} эссе\")\n",
        "print(f\"Количество батчей в train_loader: {len(train_loader)}\")\n",
        "print(f\"Количество батчей в test_loader: {len(test_loader)}\")\n",
        "\n",
        "# Проверка одного батча\n",
        "for texts, labels in train_loader:\n",
        "    print(f\"\\nПример батча из train_loader:\")\n",
        "    print(f\"    Тип текстов: {type(texts)}, количество: {len(texts)}\")\n",
        "    print(f\"    Первый текст: '{texts[0][:100]}...'\")\n",
        "    print(f\"    Тип меток: {type(labels)}, количество: {len(labels)}\")\n",
        "    print(f\"    Первая метка: {labels[0]}\")\n",
        "    break\n",
        "\n",
        "print(\"\\nЭтап 4: Подготовка данных для обучения завершена.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BlAiIFK3E5a",
        "outputId": "e006443a-e1cb-4834-b956-a45dc7825d7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер тренировочного набора: 1102 эссе\n",
            "Размер валидационного набора: 276 эссе\n",
            "Количество батчей в train_loader: 69\n",
            "Количество батчей в test_loader: 9\n",
            "\n",
            "Пример батча из train_loader:\n",
            "    Тип текстов: <class 'tuple'>, количество: 16\n",
            "    Первый текст: 'The using of cars has caused much of the worlds green house gas imitions, in America as much as 50% ...'\n",
            "    Тип меток: <class 'torch.Tensor'>, количество: 16\n",
            "    Первая метка: 0\n",
            "\n",
            "Этап 4: Подготовка данных для обучения завершена.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Defining the Generator Model"
      ],
      "metadata": {
        "id": "CCyasPVU36DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BertConfig для BertEncoder в генераторе и дискриминаторе.\n",
        "# Используем параметры из предобученной модели, чтобы BertEncoder был совместим.\n",
        "config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=pretrained_model.config.hidden_size, # 768 для bert-base-uncased\n",
        "    num_hidden_layers=num_hidden_layers, # Количество слоев, которые мы хотим использовать\n",
        "    num_attention_heads=pretrained_model.config.num_attention_heads,\n",
        "    intermediate_size=pretrained_model.config.intermediate_size,\n",
        "    hidden_act=pretrained_model.config.hidden_act,\n",
        "    hidden_dropout_prob=pretrained_model.config.hidden_dropout_prob,\n",
        "    attention_probs_dropout_prob=pretrained_model.config.attention_probs_dropout_prob,\n",
        "    max_position_embeddings=128, # Явно задаем максимальную длину последовательности\n",
        "    type_vocab_size=pretrained_model.config.type_vocab_size,\n",
        "    initializer_range=pretrained_model.config.initializer_range,\n",
        "    layer_norm_eps=pretrained_model.config.layer_norm_eps,\n",
        "    position_embedding_type='absolute',\n",
        "    use_cache=True,\n",
        "    classifier_dropout=None\n",
        ")\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        # Целевая длина последовательности для эмбеддингов BERT\n",
        "        self.output_sequence_length = 128\n",
        "        # Размерность эмбеддингов BERT (768 для bert-base-uncased)\n",
        "        self.embedding_dim = pretrained_model.config.hidden_size\n",
        "\n",
        "        # Полносвязный слой, который преобразует nz в больший тензор\n",
        "        # Его выход будет reshaped в (batch_size, initial_channels, initial_seq_len)\n",
        "        # Здесь 256 - произвольное количество каналов для начала сверточных слоев.\n",
        "        # initial_seq_len (например, 8) будет масштабироваться ConvTranspose1d до self.output_sequence_length.\n",
        "        initial_seq_len = 8 # Например, начинаем с небольшой длины\n",
        "        self.fc = nn.Linear(input_dim, 256 * initial_seq_len) # TODO\n",
        "\n",
        "        self.conv_net = nn.Sequential(\n",
        "            # Input: (batch_size, 256, 8) after fc and reshape\n",
        "            # Upsample sequence length to 16\n",
        "            nn.ConvTranspose1d(in_channels=256, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(True),\n",
        "            # Upsample sequence length to 32\n",
        "            nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(True),\n",
        "            # Upsample sequence length to 64\n",
        "            nn.ConvTranspose1d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(True),\n",
        "            # Upsample sequence length to 128\n",
        "            nn.ConvTranspose1d(in_channels=64, out_channels=self.embedding_dim, kernel_size=4, stride=2, padding=1), # Output 768 channels (embedding_dim)\n",
        "            nn.BatchNorm1d(self.embedding_dim),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        # BertEncoder будет обрабатывать сгенерированные эмбеддинги\n",
        "        self.bert_encoder = BertEncoder(config)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Преобразование латентного вектора\n",
        "        x = self.fc(x)\n",
        "        # Reshape для ConvTranspose1d: (batch_size, channels, sequence_length)\n",
        "        # Используем .view() для изменения формы тензора\n",
        "        x = x.view(x.size(0), 256, 8) # TODO - initial_seq_len (8)\n",
        "        # Пропуск через сверточную сеть\n",
        "        x = self.conv_net(x)\n",
        "        # Permute (транспонирование) для приведения к форме (batch_size, sequence_length, embedding_dim)\n",
        "        # BertEncoder ожидает такую форму\n",
        "        x = x.permute(0, 2, 1) # TODO\n",
        "\n",
        "        # Пропуск через BertEncoder для придания \"BERT-подобных\" свойств\n",
        "        # .last_hidden_state содержит выходные эмбеддинги из последнего слоя BertEncoder\n",
        "        x = self.bert_encoder(x).last_hidden_state # TODO\n",
        "        return x\n",
        "\n",
        "# Проверка генератора\n",
        "test_noise = torch.randn(train_batch_size, nz, device=device)\n",
        "test_generator = Generator(nz).to(device)\n",
        "generated_embeddings = test_generator(test_noise)\n",
        "print(f\"Размер сгенерированных эмбеддингов: {generated_embeddings.shape}\") # Должно быть (batch_size, 128, 768)\n",
        "\n",
        "print(\"\\nЭтап 5: Определение модели Генератора завершено.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnZVLp7K372y",
        "outputId": "cac7fc96-f42d-443b-b6f9-1700535ce5be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер сгенерированных эмбеддингов: torch.Size([16, 128, 768])\n",
            "\n",
            "Этап 5: Определение модели Генератора завершено.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Defining the Discriminator Model"
      ],
      "metadata": {
        "id": "a7mVIjhj4CBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "## Этап 6: Определение модели Дискриминатора\n",
        "# ---\n",
        "\n",
        "class SumBertPooler(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        sum_hidden = hidden_states.sum(dim=1)\n",
        "        sum_mask = sum_hidden.abs().sum(1).unsqueeze(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "\n",
        "        mean_embeddings = sum_hidden / sum_mask\n",
        "        return mean_embeddings\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert_encoder = BertEncoder(config)\n",
        "        self.bert_encoder.layer = nn.ModuleList([\n",
        "            layer for layer in pretrained_model.bert.encoder.layer[:num_hidden_layers]\n",
        "        ])\n",
        "        self.pooler = SumBertPooler()\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            nn.Linear(pretrained_model.config.hidden_size, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        out = self.bert_encoder(input_embeddings).last_hidden_state\n",
        "        out = self.pooler(out)\n",
        "        out = self.classifier(out)\n",
        "        return torch.sigmoid(out).view(-1)\n",
        "\n",
        "# Инициализация основной модели дискриминатора\n",
        "# ВМЕСТО `test_discriminator = Discriminator().to(device)`\n",
        "# Используем `model = Discriminator().to(device)`\n",
        "model = Discriminator().to(device)\n",
        "\n",
        "# Проверка дискриминатора (теперь используем `model`)\n",
        "# test_noise должен быть определен из Этапа 5 для этой проверки.\n",
        "# Если `generated_embeddings` уже определен из предыдущего запуска Этапа 5, используй его.\n",
        "# Если нет, пересоздай его:\n",
        "# test_noise = torch.randn(train_batch_size, nz, device=device)\n",
        "# generated_embeddings = test_generator(test_noise) # test_generator должен быть инициализирован\n",
        "# output_discriminator = model(generated_embeddings)\n",
        "\n",
        "# Проверка, что модель существует и может обрабатывать входные данные\n",
        "try:\n",
        "    # Убедимся, что test_noise и test_generator доступны, или создадим их заново\n",
        "    if 'test_noise' not in locals() or 'test_generator' not in locals():\n",
        "        test_noise = torch.randn(train_batch_size, nz, device=device)\n",
        "        # Assuming Generator class and nz are defined.\n",
        "        # This part ensures that generated_embeddings is available for testing the Discriminator\n",
        "        # You would typically define Generator and test_generator in Step 5.\n",
        "        # If test_generator is not defined yet, this would be `generator = Generator(nz).to(device)`\n",
        "        # and then `generated_embeddings = generator(test_noise)`\n",
        "        # For this specific check, we can just make dummy embeddings if needed.\n",
        "        dummy_embeddings = torch.randn(train_batch_size, 128, pretrained_model.config.hidden_size).to(device)\n",
        "        output_discriminator = model(dummy_embeddings)\n",
        "    else:\n",
        "        output_discriminator = model(generated_embeddings) # Используем сгенерированные ранее эмбеддинги\n",
        "\n",
        "    print(f\"Размер выхода дискриминатора: {output_discriminator.shape}\") # Должно быть (batch_size,)\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при проверке дискриминатора: {e}\")\n",
        "\n",
        "print(\"\\nЭтап 6: Определение модели Дискриминатора завершено.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9kzvFBA4G4Z",
        "outputId": "13f16b11-6581-474a-aa19-ff7fa3513cf6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер выхода дискриминатора: torch.Size([16])\n",
            "\n",
            "Этап 6: Определение модели Дискриминатора завершено.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Define support functions for training and assessment"
      ],
      "metadata": {
        "id": "DWOohMfJ4PJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для оценки AUC\n",
        "def eval_auc(model):\n",
        "    model.eval() # Переводим модель в режим оценки (отключаем dropout и Batch Norm для инференса)\n",
        "\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad(): # Отключаем расчет градиентов для оценки, экономит память и ускоряет\n",
        "        for batch in test_loader:\n",
        "            texts, label = batch # Разделяем батч на тексты и метки\n",
        "            # Токенизируем тексты из батча\n",
        "            encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\") # TODO\n",
        "            input_ids = encodings['input_ids'].to(device) # Перемещаем на устройство\n",
        "            token_type_ids = encodings['token_type_ids'].to(device) # Перемещаем на устройство\n",
        "            attention_mask = encodings['attention_mask'].to(device) # Перемещаем на устройство\n",
        "\n",
        "\n",
        "            embeded = embedding_model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
        "\n",
        "\n",
        "            label = label.float().to(device) # Переводим метки в float и на устройство\n",
        "\n",
        "            outputs = model(embeded) # Пропускаем эмбеддинги через дискриминатор\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(label.cpu().numpy())\n",
        "\n",
        "    auc = roc_auc_score(actuals, predictions) # TODO - Вычисляем AUC-ROC\n",
        "    return auc\n",
        "\n",
        "# Функция для сохранения информации о модели (для отслеживания лучшей версии)\n",
        "def get_model_info_dict(model, epoch, auc_score):\n",
        "    current_device = next(model.parameters()).device\n",
        "    model.to('cpu') # Переносим модель на CPU перед сохранением state_dict\n",
        "\n",
        "    model_info = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'auc_score': auc_score,\n",
        "    }\n",
        "\n",
        "    model.to(current_device) # Возвращаем модель на исходное устройство\n",
        "    return model_info\n",
        "\n",
        "# Функция для подготовки эмбеддингов текста из списка строк\n",
        "def preparation_embedding(texts):\n",
        "    # Токенизируем тексты\n",
        "    encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    # Перемещаем тензоры на устройство\n",
        "    input_ids = encodings['input_ids'].to(device) # TODO\n",
        "    token_type_ids = encodings['token_type_ids'].to(device) # TODO\n",
        "    # Получаем эмбеддинги. Как и в eval_auc, embedding_model возвращает тензор напрямую.\n",
        "    embeded = embedding_model(input_ids=input_ids, token_type_ids=token_type_ids) # TODO\n",
        "    return embeded\n",
        "\n",
        "print(\"Этап 7: Определение вспомогательных функций завершено.\")\n",
        "\n",
        "\n",
        "# Проверка функции preparation_embedding\n",
        "try:\n",
        "    print(\"\\nТестирование preparation_embedding...\")\n",
        "    sample_texts_for_embedding = [\n",
        "        \"This is a short test sentence for embedding.\",\n",
        "        \"Another example text to check the output dimensions.\"\n",
        "    ]\n",
        "    # Используем `sample_texts_for_embedding` для получения эмбеддингов\n",
        "    test_embeddings = preparation_embedding(sample_texts_for_embedding)\n",
        "    print(f\"Размер эмбеддингов для sample_texts: {test_embeddings.shape}\")\n",
        "    # Ожидаемый размер: (batch_size, max_length, hidden_size)\n",
        "    # Для bert-base-uncased, hidden_size = 768, max_length = 128\n",
        "    # Так что ожидаем что-то вроде (2, 128, 768)\n",
        "    if test_embeddings.shape[0] == len(sample_texts_for_embedding) and test_embeddings.shape[2] == 768:\n",
        "        print(\"preparation_embedding: Успех! Размер эмбеддингов соответствует ожиданиям.\")\n",
        "    else:\n",
        "        print(\"preparation_embedding: Предупреждение! Размер эмбеддингов не совсем соответствует ожиданиям.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при тестировании preparation_embedding: {e}\")\n",
        "    print(\"Убедитесь, что 'tokenizer' и 'embedding_model' корректно инициализированы.\")\n",
        "\n",
        "\n",
        "# Проверка функции eval_auc\n",
        "try:\n",
        "    print(\"\\nТестирование eval_auc...\")\n",
        "    # Чтобы eval_auc работал, test_loader должен быть заполнен, а model - это дискриминатор\n",
        "    # (даже если это заглушка DummyDiscriminator, главное, чтобы он принимал эмбеддинги\n",
        "    # и выдавал одно число на выходе).\n",
        "    test_auc_score = eval_auc(model)\n",
        "    print(f\"Предполагаемый AUC score на тестовом наборе: {test_auc_score:.4f}\")\n",
        "    print(\"eval_auc: Успех! Функция отработала без ошибок.\")\n",
        "    print(\"Примечание: Значение AUC будет случайным, если модель не обучена.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при тестировании eval_auc: {e}\")\n",
        "    print(\"Убедитесь, что 'model' и 'test_loader' корректно инициализированы и их типы данных совместимы.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Проверки функций Этапа 7 завершены. ---\")\n",
        "\n",
        "\n",
        "# ---\n",
        "## Проверка функций после Этапа 7\n",
        "# ---\n",
        "\n",
        "print(\"\\n--- Проверка функций Этапа 7 ---\")\n",
        "\n",
        "# Проверка функции preparation_embedding\n",
        "try:\n",
        "    print(\"\\nТестирование preparation_embedding...\")\n",
        "    sample_texts_for_embedding = [\n",
        "        \"This is a short test sentence for embedding.\",\n",
        "        \"Another example text to check the output dimensions.\"\n",
        "    ]\n",
        "    # Используем `sample_texts_for_embedding` для получения эмбеддингов\n",
        "    test_embeddings = preparation_embedding(sample_texts_for_embedding)\n",
        "    print(f\"Размер эмбеддингов для sample_texts: {test_embeddings.shape}\")\n",
        "    # Ожидаемый размер: (batch_size, max_length, hidden_size)\n",
        "    # Для bert-base-uncased, hidden_size = 768, max_length = 128\n",
        "    # Так что ожидаем что-то вроде (2, 128, 768)\n",
        "    if test_embeddings.shape[0] == len(sample_texts_for_embedding) and test_embeddings.shape[2] == 768:\n",
        "        print(\"preparation_embedding: Успех! Размер эмбеддингов соответствует ожиданиям.\")\n",
        "    else:\n",
        "        print(\"preparation_embedding: Предупреждение! Размер эмбеддингов не совсем соответствует ожиданиям.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при тестировании preparation_embedding: {e}\")\n",
        "    print(\"Убедитесь, что 'tokenizer' и 'embedding_model' корректно инициализированы.\")\n",
        "\n",
        "\n",
        "# Проверка функции eval_auc\n",
        "try:\n",
        "    print(\"\\nТестирование eval_auc...\")\n",
        "    # Чтобы eval_auc работал, test_loader должен быть заполнен, а model - это дискриминатор\n",
        "    # (даже если это заглушка DummyDiscriminator, главное, чтобы он принимал эмбеддинги\n",
        "    # и выдавал одно число на выходе).\n",
        "    test_auc_score = eval_auc(model)\n",
        "    print(f\"Предполагаемый AUC score на тестовом наборе: {test_auc_score:.4f}\")\n",
        "    print(\"eval_auc: Успех! Функция отработала без ошибок.\")\n",
        "    print(\"Примечание: Значение AUC будет случайным, если модель не обучена.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при тестировании eval_auc: {e}\")\n",
        "    print(\"Убедитесь, что 'model' и 'test_loader' корректно инициализированы и их типы данных совместимы.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Проверки функций Этапа 7 завершены. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0Ge5Shq4RQR",
        "outputId": "da5eeda6-350b-40c1-c822-7cc7c8d5671c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Этап 7: Определение вспомогательных функций завершено.\n",
            "\n",
            "Тестирование preparation_embedding...\n",
            "Размер эмбеддингов для sample_texts: torch.Size([2, 128, 768])\n",
            "preparation_embedding: Успех! Размер эмбеддингов соответствует ожиданиям.\n",
            "\n",
            "Тестирование eval_auc...\n",
            "Предполагаемый AUC score на тестовом наборе: 0.6073\n",
            "eval_auc: Успех! Функция отработала без ошибок.\n",
            "Примечание: Значение AUC будет случайным, если модель не обучена.\n",
            "\n",
            "--- Проверки функций Этапа 7 завершены. ---\n",
            "\n",
            "--- Проверка функций Этапа 7 ---\n",
            "\n",
            "Тестирование preparation_embedding...\n",
            "Размер эмбеддингов для sample_texts: torch.Size([2, 128, 768])\n",
            "preparation_embedding: Успех! Размер эмбеддингов соответствует ожиданиям.\n",
            "\n",
            "Тестирование eval_auc...\n",
            "Предполагаемый AUC score на тестовом наборе: 0.6073\n",
            "eval_auc: Успех! Функция отработала без ошибок.\n",
            "Примечание: Значение AUC будет случайным, если модель не обучена.\n",
            "\n",
            "--- Проверки функций Этапа 7 завершены. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: GAN Training Cycle"
      ],
      "metadata": {
        "id": "BmIvIbwP5yUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GAN_step(optimizerG, optimizerD, netG, netD, real_data_embeddings, label_template, epoch, i):\n",
        "    # label_template используется для создания новых тензоров меток с правильным размером и устройством\n",
        "\n",
        "    # (1) Обновление D сети: Максимизация log(D(x)) + log(1 - D(G(z)))\n",
        "    netD.zero_grad() # Обнуляем градиенты дискриминатора\n",
        "    batch_size = real_data_embeddings.size(0)\n",
        "\n",
        "    # Обучение на реальных данных\n",
        "    label_real = torch.full((batch_size,), 1.0, dtype=torch.float, device=device) # Метки 1 для реальных данных # TODO\n",
        "    output_real = netD(real_data_embeddings).view(-1)\n",
        "    errD_real = criterion(output_real, label_real)\n",
        "    errD_real.backward() # Обратное распространение ошибки для реальных данных\n",
        "    D_x = output_real.mean().item() # Средний выход дискриминатора для реальных\n",
        "\n",
        "    # Обучение на фейковых данных, сгенерированных G\n",
        "    noise = torch.randn(batch_size, nz, device=device) # Генерируем случайный шум\n",
        "    fake_data_embeddings = netG(noise) # Генерируем фейковые эмбеддинги\n",
        "    # Метки 0 для фейковых данных\n",
        "    label_fake = torch.full((batch_size,), 0.0, dtype=torch.float, device=device) # TODO\n",
        "    # Важно: .detach() отсоединяет фейковые данные от графа вычислений Генератора,\n",
        "    # чтобы при обновлении Дискриминатора градиенты не текли обратно к Генератору.\n",
        "    output_fake = netD(fake_data_embeddings.detach()).view(-1)\n",
        "    errD_fake = criterion(output_fake, label_fake)\n",
        "    errD_fake.backward() # Обратное распространение ошибки для фейковых данных\n",
        "    D_G_z1 = output_fake.mean().item() # Средний выход дискриминатора для фейковых (до обновления G)\n",
        "    errD = errD_real + errD_fake # Общая ошибка дискриминатора\n",
        "    optimizerD.step() # Обновляем параметры дискриминатора\n",
        "\n",
        "    # (2) Обновление G сети: Максимизация log(D(G(z)))\n",
        "    netG.zero_grad() # Обнуляем градиенты генератора\n",
        "    # Генератор хочет, чтобы дискриминатор считал его выход реальным (т.е., метка 1)\n",
        "    label_gen = torch.full((batch_size,), 1.0, dtype=torch.float, device=device) # TODO\n",
        "    output_gen = netD(fake_data_embeddings).view(-1) # Пропускаем фейковые данные через дискриминатор снова\n",
        "    errG = criterion(output_gen, label_gen) # Ошибка генератора (насколько хорошо он обманул D)\n",
        "    errG.backward() # Обратное распространение ошибки для генератора\n",
        "    D_G_z2 = output_gen.mean().item() # Средний выход дискриминатора для фейковых (после обновления G)\n",
        "    optimizerG.step() # Обновляем параметры генератора\n",
        "\n",
        "    if i % 50 == 0: # Печатаем прогресс каждые 50 итераций\n",
        "        print(f'[{epoch+1}/{num_epochs}][{i}/{len(train_loader)}] '\n",
        "              f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
        "              f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
        "\n",
        "    return optimizerG, optimizerD, netG, netD\n",
        "\n",
        "# Инициализируем Генератор и Дискриминатор\n",
        "netG = Generator(nz).to(device) # TODO\n",
        "netD = Discriminator().to(device) # TODO\n",
        "\n",
        "# Определяем функцию потерь (Binary Cross-Entropy Loss)\n",
        "criterion = nn.BCELoss() # TODO\n",
        "\n",
        "# Определяем оптимизаторы (Adam)\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) # TODO\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)) # TODO\n",
        "\n",
        "model_infos = [] # Список для хранения информации о моделях (для выбора лучшей по AUC)\n",
        "best_auc = 0.0 # Отслеживание лучшего AUC\n",
        "max_auc_model_info = None # Хранение state_dict лучшей модели\n",
        "\n",
        "print(\"Начало обучения GAN...\")\n",
        "for epoch in range(num_epochs):\n",
        "    netG.train() # Переводим генератор в режим обучения\n",
        "    netD.train() # Переводим дискриминатор в режим обучения\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # data[0] - это тексты (str), data[1] - это метки (int)\n",
        "\n",
        "        with torch.no_grad(): # Градиенты для эмбеддингов не нужны\n",
        "            # Подготовка эмбеддингов реальных данных\n",
        "            # data[0] содержит тексты, которые нужно токенизировать и эмбеддировать\n",
        "            real_data_embeddings = preparation_embedding(data[0])\n",
        "\n",
        "        optimizerG, optimizerD, netG, netD = GAN_step(\n",
        "            optimizerG=optimizerG, # TODO\n",
        "            optimizerD=optimizerD, # TODO\n",
        "            netG=netG,\n",
        "            netD=netD,\n",
        "            real_data_embeddings=real_data_embeddings, # TODO\n",
        "            label_template=data[1].float().to(device), # TODO - Используем метки из даталоадера как шаблон\n",
        "            epoch=epoch, i=i)\n",
        "\n",
        "    # Оценка модели (Дискриминатора) после каждой эпохи\n",
        "    auc_score = eval_auc(netD) # TODO\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} --- Validation AUC: {auc_score:.4f} ---\")\n",
        "\n",
        "    current_model_info = get_model_info_dict(netD, epoch, auc_score)\n",
        "    if auc_score > best_auc: # Если текущий AUC лучше предыдущего лучшего\n",
        "        best_auc = auc_score\n",
        "        max_auc_model_info = current_model_info # Обновляем информацию о лучшей модели\n",
        "\n",
        "    model_infos.append(current_model_info)\n",
        "\n",
        "print('Обучение GAN завершено！')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIaJ7zjY5z2B",
        "outputId": "23ce346e-d547-40de-8c08-f9bfc66baa7d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начало обучения GAN...\n",
            "[1/5][0/69] Loss_D: 1.3879 Loss_G: 0.7357 D(x): 0.4794 D(G(z)): 0.4794 / 0.4792\n",
            "[1/5][50/69] Loss_D: 1.3086 Loss_G: 0.7550 D(x): 0.5101 D(G(z)): 0.4703 / 0.4700\n",
            "\n",
            "--- Epoch 1/5 --- Validation AUC: 0.3164 ---\n",
            "[2/5][0/69] Loss_D: 1.2630 Loss_G: 0.7722 D(x): 0.5261 D(G(z)): 0.4625 / 0.4620\n",
            "[2/5][50/69] Loss_D: 1.1003 Loss_G: 0.8637 D(x): 0.5763 D(G(z)): 0.4226 / 0.4216\n",
            "\n",
            "--- Epoch 2/5 --- Validation AUC: 0.9200 ---\n",
            "[3/5][0/69] Loss_D: 1.0235 Loss_G: 0.9191 D(x): 0.5992 D(G(z)): 0.4003 / 0.3989\n",
            "[3/5][50/69] Loss_D: 0.8091 Loss_G: 1.1155 D(x): 0.6642 D(G(z)): 0.3296 / 0.3278\n",
            "\n",
            "--- Epoch 3/5 --- Validation AUC: 0.0364 ---\n",
            "[4/5][0/69] Loss_D: 0.7281 Loss_G: 1.2069 D(x): 0.6905 D(G(z)): 0.3008 / 0.2991\n",
            "[4/5][50/69] Loss_D: 0.5423 Loss_G: 1.4663 D(x): 0.7568 D(G(z)): 0.2317 / 0.2308\n",
            "\n",
            "--- Epoch 4/5 --- Validation AUC: 0.0291 ---\n",
            "[5/5][0/69] Loss_D: 0.4827 Loss_G: 1.5708 D(x): 0.7800 D(G(z)): 0.2088 / 0.2079\n",
            "[5/5][50/69] Loss_D: 0.3545 Loss_G: 1.8540 D(x): 0.8322 D(G(z)): 0.1571 / 0.1566\n",
            "\n",
            "--- Epoch 5/5 --- Validation AUC: 0.3673 ---\n",
            "Обучение GAN завершено！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Inference and Submission File Creation"
      ],
      "metadata": {
        "id": "wjFtxCiZA0hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "## Этап 9: Инференс и создание файла submission\n",
        "# ---\n",
        "\n",
        "print(\"\\nНачало этапа инференса...\")\n",
        "\n",
        "# Проверка наличия max_auc_model_info и model_infos\n",
        "if max_auc_model_info is None:\n",
        "    print(\"Внимание: max_auc_model_info не был установлен. Возможно, обучение не завершилось или AUC не рос.\")\n",
        "    print(\"Использую информацию о последней обученной модели для инференса.\")\n",
        "    if model_infos:\n",
        "        # Берем последнюю сохраненную модель, если лучшая не найдена\n",
        "        max_auc_model_info = model_infos[-1]\n",
        "    else:\n",
        "        # Если model_infos пуст, значит, обучение не производилось вовсе\n",
        "        raise ValueError(\"Нет доступной информации о моделях для инференса. Обучение, возможно, не производилось.\")\n",
        "\n",
        "# Загружаем лучшую модель дискриминатора\n",
        "# model уже был инициализирован как Discriminator().to(device) в Этапе 6.\n",
        "# Можно перезагрузить его state_dict, или создать новый экземпляр, как ты и делаешь здесь.\n",
        "# Создание нового экземпляра:\n",
        "inference_model = Discriminator().to(device) # Создаем новый экземпляр дискриминатора для инференса\n",
        "inference_model.load_state_dict(max_auc_model_info['model_state_dict']) # Загружаем сохраненные веса\n",
        "inference_model.eval() # Переводим модель в режим оценки (отключаем дропаут и BatchNorm для инференса)\n",
        "\n",
        "print(f\"Лучшая модель Дискриминатора (AUC: {max_auc_model_info.get('auc_score', 'N/A'):.4f}) загружена.\")\n",
        "\n",
        "# Подготовка данных для инференса из src_test_essays\n",
        "# Убедитесь, что src_test_essays загружен на Этапе 1\n",
        "inference_texts = src_test_essays['text'].tolist()\n",
        "inference_ids = src_test_essays['id'].tolist()\n",
        "\n",
        "class InferenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "sub_dataset = InferenceDataset(inference_texts)\n",
        "\n",
        "# Используем test_batch_size, определенный в Этапе 3\n",
        "inference_loader = DataLoader(sub_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "sub_predictions = []\n",
        "with torch.no_grad(): # Отключаем градиенты\n",
        "    for batch_texts in inference_loader:\n",
        "        # Токенизируем тексты\n",
        "        encodings = tokenizer(batch_texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "        input_ids = encodings['input_ids'].to(device)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device)\n",
        "        # Получаем эмбеддинги. ВНИМАНИЕ: УБРАНО .last_hidden_state\n",
        "        embeded = embedding_model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
        "\n",
        "        outputs = inference_model(embeded) # Пропускаем через загруженную модель дискриминатора\n",
        "        sub_predictions.extend(outputs.cpu().numpy()) # Сохраняем предсказания\n",
        "\n",
        "# Создаем DataFrame для submission файла\n",
        "# sub_predictions уже должен быть одномерным массивом благодаря .view(-1) в Discriminator\n",
        "sub_ans_df = pd.DataFrame({'id': inference_ids, 'score': sub_predictions})\n",
        "\n",
        "print(\"\\nПример финального submission DataFrame:\")\n",
        "print(sub_ans_df.head())\n",
        "\n",
        "# Сохранение submission файла\n",
        "submission_filename = 'submission.csv'\n",
        "sub_ans_df.to_csv(submission_filename, index=False)\n",
        "print(f\"\\nФайл {submission_filename} успешно создан.\")\n",
        "\n",
        "print(\"\\nЭтап 9: Инференс и создание файла submission завершен.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irVUWDoDA2W_",
        "outputId": "6cefb1b0-453e-44a7-f73a-42089970d4b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Начало этапа инференса...\n",
            "Лучшая модель Дискриминатора (AUC: 0.9200) загружена.\n",
            "\n",
            "Пример финального submission DataFrame:\n",
            "         id     score\n",
            "0  0000aaaa  0.598717\n",
            "1  1111bbbb  0.598734\n",
            "2  2222cccc  0.598745\n",
            "\n",
            "Файл submission.csv успешно создан.\n",
            "\n",
            "Этап 9: Инференс и создание файла submission завершен.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUM"
      ],
      "metadata": {
        "id": "9i3OR2inBbl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Prepare environment and imports. Set up the Colab environment and downloaded all necessary libraries.\n",
        "\n",
        "Step 1: Load data. Ensured that the competition data loads correctly.\n",
        "\n",
        "Step 2: Prepare BERT model and tokenizer. Initialized the tokenizer and extracted the BERT embedding layer for use in the GAN.\n",
        "\n",
        "Step 3: Define hyperparameters. Centrally defined all key parameters for training the model.\n",
        "\n",
        "Step 4: Prepare training data. Created a custom Dataset and DataLoader to efficiently feed data to the model. Fixed the issue with duplicate hyperparameters.\n",
        "\n",
        "Step 5: Define the Generator model. Designed the Generator architecture that transforms random noise into BERT-like embeddings.\n",
        "\n",
        "Step 6: Define the Discriminator model. Created a Discriminator that uses a BERT encoder to classify embeddings as \"real\" or \"fake\". Ensured that the model is globally accessible.\n",
        "\n",
        "Step 7: Helper functions. Implemented functions for AUC estimation and embedding preparation, eliminating the nuances of last_hidden_state.\n",
        "\n",
        "Step 8: GAN training loop. Set up the logic for step-by-step training of the Generator and Discriminator, including loss functions and optimizers.\n",
        "\n",
        "Step 9: Inference and submission file generation. Successfully made predictions on the test dataset and generated a submission.csv file with probabilities for each essay."
      ],
      "metadata": {
        "id": "Qhs0tCkXBht-"
      }
    }
  ]
}